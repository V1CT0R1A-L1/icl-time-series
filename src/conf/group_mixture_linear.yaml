inherit:
  - models/standard.yaml
  - wandb.yaml

model:
  n_dims: 5        # input dimension
  n_positions: 80   # need max_allowed_length = n_positions // 2 >= 20, so n_positions >= 40 (using 50 for safety)

training:
  task: group_mixture_linear
  data: group_mixture_linear
  task_kwargs:
    n_components: 2            # K (number of components)
    contexts_per_component: 10  # C (points per context cluster)
    target_cluster_context_points: 6  # T (points with y-values in target cluster for inference)
    noise_std: 0.0
    scale: 0.5                # target scale; with normalize_ys false, loss can drop as model learns (normalize_ys true often keeps loss ~1 if read_out init was too small)
    normalize_ys: false       # set true to report MSE in normalized space; false lets model learn the raw mapping and usually gives decreasing loss
    predict_target_only: true  # standard ICL: predict only the target query (last position) given full context; report MSE on that position
  batch_size: 64
  learning_rate: 0.003
  save_every_steps: 1000
  keep_every_steps: 100000
  train_steps: 5001
  curriculum:
    dims:
      start: 20
      end: 20
      inc: 1
      interval: 2000
    points:
      start: 20   # T = (K * C) + (T_target + 1) = (2 * 8) + (3 + 1) = 16 + 4 = 20
      end: 20
      inc: 2
      interval: 2000

out_dir: ../models/group_mixture_linear

wandb:
  name: "group_mixture_linear"

test_run: False

