inherit:
  - models/standard.yaml
  - wandb.yaml

model:
  n_dims: 5        # input dimension
  n_positions: 80   # need max_allowed_length = n_positions // 2 >= 20, so n_positions >= 40 (using 50 for safety)
  use_pairs_format: true  # one token per (x,y), last token = [x_query; 0]; standard ICL layout, often learns better than interleaved

training:
  task: group_mixture_linear
  data: group_mixture_linear
  task_kwargs:
    n_components: 2            # K (number of components)
    contexts_per_component: 10  # C (points per context cluster)
    target_cluster_context_points: 6  # T (points with y-values in target cluster for inference)
    noise_std: 0.0
    scale: 0.5                # target scale
    normalize_ys: false       # set true to report MSE in normalized space
    # Switch: true = predict ONE position (target query only). false = predict ALL positions.
    predict_target_only: false
  batch_size: 64
  learning_rate: 0.0005   # lower LR so loss can descend to ~0 in noiseless case (0.003 often plateaus at ~0.3)
  lr_warmup_steps: 300    # linear warmup; helps loss descend smoothly in noiseless ICL
  save_every_steps: 1000
  keep_every_steps: 100000
  train_steps: 5001
  curriculum:
    dims:
      start: 20
      end: 20
      inc: 1
      interval: 2000
    points:
      start: 20   # T = (K * C) + (T_target + 1) = (2 * 8) + (3 + 1) = 16 + 4 = 20
      end: 20
      inc: 2
      interval: 2000

out_dir: ../models/group_mixture_linear

wandb:
  name: "group_mixture_linear"

test_run: False

