{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2702f0fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "VENV_PATH = \"/orcd/home/002/v1ct0r1a/icl-time-series/in-context-learning\"\n",
        "PYTHON_VERSION = \"python3.10\"\n",
        "\n",
        "SITE_PACKAGES_PATH = os.path.join(VENV_PATH, 'lib', PYTHON_VERSION, 'site-packages')\n",
        "\n",
        "if SITE_PACKAGES_PATH not in sys.path:\n",
        "    sys.path.append(SITE_PACKAGES_PATH)\n",
        "    print(f\"Added VENV site-packages to path: {SITE_PACKAGES_PATH}\")\n",
        "else:\n",
        "    print(\"VENV site-packages path already present.\")\n",
        "\n",
        "SRC_DIR = \"/orcd/home/002/v1ct0r1a/icl-time-series/src\"\n",
        "\n",
        "if SRC_DIR not in sys.path:\n",
        "    sys.path.append(SRC_DIR)\n",
        "    print(f\"Added src directory to path: {SRC_DIR}\")\n",
        "else:\n",
        "    print(\"src directory path already present.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6cfeb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/eddieqiao/miniconda3/envs/in-context-learning/lib/python3.11/site-packages/munch/__init__.py:24: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "import re\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
        "from plot_utils import basic_plot, collect_results, relevant_model_names\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "sns.set_theme('notebook', 'darkgrid')\n",
        "palette = sns.color_palette('colorblind')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9980951",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "run_dir = \"/home/v1ct0r1a/models/group_mixture_linear\"\n",
        "task = \"group_mixture_linear\" \n",
        "run_id = \"0b19bd1f-2c8d-4dc2-89bb-f0bcdf529454\"\n",
        "\n",
        "run_path = os.path.join(run_dir, run_id)\n",
        "recompute_metrics = False\n",
        "\n",
        "if recompute_metrics:\n",
        "    get_run_metrics(run_path)  # these are normally precomputed at the end of training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71dc5b5",
      "metadata": {},
      "source": [
        "# Interactive setup\n",
        "\n",
        "We will now directly load the model and measure its in-context learning ability on a batch of random inputs. (In the paper we average over multiple such batches to obtain better estimates.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e55bb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from samplers import get_data_sampler\n",
        "from tasks import get_task_sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fcb390",
      "metadata": {},
      "outputs": [],
      "source": [
        "model, conf = get_model_from_run(run_path)\n",
        "n_dims = conf.model.n_dims\n",
        "batch_size = conf.training.batch_size\n",
        "\n",
        "\n",
        "# Convert task_kwargs properly\n",
        "task_kwargs = {}\n",
        "if hasattr(conf.training, 'task_kwargs') and conf.training.task_kwargs:\n",
        "    if hasattr(conf.training.task_kwargs, '__dict__'):\n",
        "        task_kwargs = conf.training.task_kwargs.__dict__\n",
        "    else:\n",
        "        task_kwargs = conf.training.task_kwargs\n",
        "\n",
        "print(\"Task kwargs:\", task_kwargs)\n",
        "\n",
        "data_sampler = get_data_sampler(conf.training.data, n_dims=n_dims, **task_kwargs)\n",
        "\n",
        "task_sampler = get_task_sampler(\n",
        "    conf.training.task,\n",
        "    n_dims,\n",
        "    batch_size,\n",
        "    num_tasks=conf.training.num_tasks if hasattr(conf.training, 'num_tasks') else None,\n",
        "    **task_kwargs,\n",
        ")\n",
        "\n",
        "sequence_structure = None\n",
        "predict_inds = None\n",
        "if hasattr(data_sampler, 'get_sequence_structure'):\n",
        "    sequence_structure = data_sampler.get_sequence_structure()\n",
        "    predict_inds = sequence_structure.get('predict_inds', [])\n",
        "    print(f\"Multi-context structure: {sequence_structure}\")\n",
        "    print(f\"Will predict {len(predict_inds)} indices: {predict_inds[:10]}...\" if len(predict_inds) > 10 else f\"Will predict indices: {predict_inds}\")\n",
        "    \n",
        "    # For group_mixture_linear, we now predict all positions\n",
        "    if conf.training.task == \"group_mixture_linear\":\n",
        "        print(f\"\\nTask: {conf.training.task}\")\n",
        "        print(f\"  - Total sequence length: {sequence_structure['total_length']}\")\n",
        "        print(f\"  - Predicting ALL {len(predict_inds)} positions (autoregressive)\")\n",
        "        if hasattr(data_sampler, 'n_components'):\n",
        "            K = data_sampler.n_components\n",
        "            C = data_sampler.contexts_per_component\n",
        "            T_target = data_sampler.target_cluster_context_points\n",
        "            context_length = K * C\n",
        "            print(f\"  - Structure: {K} context clusters × {C} points + target cluster ({T_target} context + 1 prediction)\")\n",
        "            print(f\"  - Context clusters: positions 0-{context_length-1}\")\n",
        "            print(f\"  - Target cluster: positions {context_length}-{sequence_structure['total_length']-1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ea2a21",
      "metadata": {},
      "outputs": [],
      "source": [
        "if sequence_structure is not None:\n",
        "    n_points = sequence_structure['total_length']\n",
        "else:\n",
        "    n_points = conf.training.curriculum.points.end\n",
        "\n",
        "# For group_mixture_linear, we need to pass components and assignments from sampler\n",
        "task_sampler_args = {}\n",
        "if conf.training.task == \"group_mixture_linear\":\n",
        "    assert hasattr(data_sampler, \"current_components\"), \"Sampler must set current_components\"\n",
        "    assert hasattr(data_sampler, \"component_assignments\"), \"Sampler must set component_assignments\"\n",
        "    task = task_sampler(\n",
        "        components=data_sampler.current_components,\n",
        "        component_assignments=data_sampler.component_assignments,\n",
        "        **task_sampler_args,\n",
        "    )\n",
        "    xs = data_sampler.sample_xs(b_size=batch_size, n_points=n_points)\n",
        "    ys = task.evaluate(xs)\n",
        "else:\n",
        "    task = task_sampler(**task_sampler_args)\n",
        "    xs = data_sampler.sample_xs(b_size=batch_size, n_points=n_points)\n",
        "    ys = task.evaluate(xs)\n",
        "\n",
        "print(f\"Data shapes - xs: {xs.shape}, ys: {ys.shape}\")\n",
        "\n",
        "print(f\"xs stats: mean={xs.mean():.4f}, std={xs.std():.4f}\")\n",
        "print(f\"ys stats: mean={ys.mean():.4f}, std={ys.std():.4f}\")\n",
        "\n",
        "print(\"\\nFirst training example (first few points):\")\n",
        "print(f\"xs[0, :5]: {xs[0, :5]}\")\n",
        "print(f\"ys[0, :5]: {ys[0, :5]}\")\n",
        "\n",
        "# Show structure for group_mixture_linear\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'component_assignments'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    T_target = data_sampler.target_cluster_context_points\n",
        "    context_length = K * C\n",
        "    print(f\"\\nSequence structure (first example):\")\n",
        "    print(f\"  Component assignments: {data_sampler.component_assignments[0].cpu().tolist()}\")\n",
        "    print(f\"  Context clusters (0-{context_length-1}): components {data_sampler.component_assignments[0, :context_length].cpu().tolist()}\")\n",
        "    print(f\"  Target cluster ({context_length}-{n_points-1}): components {data_sampler.component_assignments[0, context_length:].cpu().tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbfc26b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/eddieqiao/miniconda3/envs/in-context-learning/lib/python3.11/site-packages/munch/__init__.py:24: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    if predict_inds is not None and len(predict_inds) > 0 and sequence_structure is not None:\n",
        "        # Check if predicting all positions (autoregressive mode)\n",
        "        if len(predict_inds) == ys.shape[1] and set(predict_inds) == set(range(ys.shape[1])):\n",
        "            # Predicting all positions: use standard autoregressive call\n",
        "            pred = model(xs, ys)\n",
        "            print(f\"Autoregressive prediction (all positions) shape: {pred.shape}\")\n",
        "        else:\n",
        "            # Predicting specific positions\n",
        "            pred = model(xs, ys, inds=predict_inds, sequence_structure=sequence_structure)\n",
        "            print(f\"Multi-context prediction shape: {pred.shape}\")\n",
        "    else:\n",
        "        pred = model(xs, ys)\n",
        "        print(f\"Standard prediction shape: {pred.shape}\")\n",
        "\n",
        "print(\"\\nPrediction examples:\")\n",
        "if predict_inds is not None and len(predict_inds) > 0:\n",
        "    # If predicting all positions, pred shape is (B, T)\n",
        "    # If predicting specific positions, pred shape is (B, len(predict_inds))\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        # All positions predicted\n",
        "        for i in range(min(3, batch_size)):\n",
        "            print(f\"Example {i} (showing first 10 positions):\")\n",
        "            print(f\"  Actual: {ys[i, :10].numpy()}\")\n",
        "            print(f\"  Pred:   {pred[i, :10].numpy()}\")\n",
        "            print(f\"  Error:  {(pred[i, :10] - ys[i, :10]).abs().numpy()}\")\n",
        "    else:\n",
        "        # Specific positions predicted\n",
        "        for i in range(min(3, batch_size)):\n",
        "            actual_targets = ys[i, predict_inds]\n",
        "            predictions = pred[i]\n",
        "            print(f\"Example {i}:\")\n",
        "            print(f\"  Actual: {actual_targets[:5].numpy()}\")\n",
        "            print(f\"  Pred:   {predictions[:5].numpy()}\")\n",
        "else:\n",
        "    for i in range(min(3, batch_size)):\n",
        "        print(f\"Example {i}: actual={ys[i, -1]:.4f}, pred={pred[i, -1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c44eb2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = task.get_metric()\n",
        "\n",
        "# Compute loss: if predicting all positions, pred and ys have same shape\n",
        "if predict_inds is not None and len(predict_inds) > 0:\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        # Predicting all positions: pred shape is (B, T), compare with ys (B, T)\n",
        "        loss = metric(pred, ys).numpy()  # (B, T)\n",
        "        print(f\"Loss computed on all {len(predict_inds)} positions\")\n",
        "    else:\n",
        "        # Predicting specific positions: pred shape is (B, len(predict_inds))\n",
        "        loss = metric(pred, ys[:, predict_inds]).numpy()\n",
        "        print(f\"Loss computed on {len(predict_inds)} prediction positions\")\n",
        "else:\n",
        "    loss = metric(pred, ys).numpy()\n",
        "\n",
        "print(f\"Loss shape: {loss.shape}\") \n",
        "print(f\"Mean loss: {loss.mean():.4f}\")\n",
        "\n",
        "# Visualize performance across all positions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: MSE by position\n",
        "if len(loss.shape) == 2 and loss.shape[1] > 1:\n",
        "    # Loss is (B, T) - compute mean across batch for each position\n",
        "    mse_by_position = loss.mean(axis=0)  # (T,)\n",
        "    positions = range(len(mse_by_position))\n",
        "    \n",
        "    axes[0].plot(positions, mse_by_position, 'o-', linewidth=2, markersize=4)\n",
        "    axes[0].set_xlabel(\"Position in Sequence\", fontsize=12)\n",
        "    axes[0].set_ylabel(\"Mean Squared Error\", fontsize=12)\n",
        "    axes[0].set_title(f'MSE by Position (Mean: {mse_by_position.mean():.4f})', fontsize=13)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add vertical line to separate context clusters from target cluster\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < len(positions):\n",
        "            axes[0].axvline(x=context_length-0.5, color='r', linestyle='--', alpha=0.5, label='Context/Target boundary')\n",
        "            axes[0].legend()\n",
        "    \n",
        "    # Plot 2: Loss distribution histogram\n",
        "    axes[1].hist(loss.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1].set_xlabel(\"Squared Error\", fontsize=12)\n",
        "    axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
        "    axes[1].set_title(f'Error Distribution (Mean: {loss.mean():.4f})', fontsize=13)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    # Single position or aggregated loss\n",
        "    axes[0].hist(loss.flatten(), bins=20, alpha=0.7, edgecolor='black')\n",
        "    axes[0].set_xlabel(\"Squared Error\", fontsize=12)\n",
        "    axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
        "    axes[0].set_title(f'Loss Distribution (Mean MSE: {loss.mean():.4f})', fontsize=13)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca9661a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Component-specific analysis for group_mixture_linear\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'component_assignments'):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"COMPONENT-SPECIFIC ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    T_target = data_sampler.target_cluster_context_points\n",
        "    context_length = K * C\n",
        "    \n",
        "    # Compute errors\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        e = (pred - ys)**2  # (B, T)\n",
        "    else:\n",
        "        e = (pred - ys[:, predict_inds])**2\n",
        "    \n",
        "    # Analyze by component\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: MSE by component (averaged across all positions where that component is used)\n",
        "    component_mse = {}\n",
        "    for comp_id in range(K):\n",
        "        # Find all positions where this component is used\n",
        "        comp_mask = data_sampler.component_assignments == comp_id  # (B, T)\n",
        "        if comp_mask.any():\n",
        "            # Average error across all positions using this component\n",
        "            comp_errors = e[comp_mask].mean().item()\n",
        "            component_mse[comp_id] = comp_errors\n",
        "    \n",
        "    if component_mse:\n",
        "        comp_ids = list(component_mse.keys())\n",
        "        comp_mses = [component_mse[cid] for cid in comp_ids]\n",
        "        axes[0].bar(comp_ids, comp_mses, alpha=0.7, edgecolor='black')\n",
        "        axes[0].set_xlabel(\"Component ID\", fontsize=12)\n",
        "        axes[0].set_ylabel(\"Mean Squared Error\", fontsize=12)\n",
        "        axes[0].set_title(f'MSE by Component (Averaged Across All Positions)', fontsize=13)\n",
        "        axes[0].grid(True, alpha=0.3, axis='y')\n",
        "        for i, (cid, mse) in enumerate(zip(comp_ids, comp_mses)):\n",
        "            axes[0].text(cid, mse, f'{mse:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    # Plot 2: MSE by position, colored by component\n",
        "    if len(e.shape) == 2 and e.shape[1] > 1:\n",
        "        mse_by_pos = e.mean(dim=0).cpu().numpy()  # (T,)\n",
        "        positions = range(len(mse_by_pos))\n",
        "        \n",
        "        # Color points by which component is used (for first example)\n",
        "        comp_assignments_ex0 = data_sampler.component_assignments[0].cpu().numpy()\n",
        "        colors = plt.cm.tab10(range(K))\n",
        "        \n",
        "        for comp_id in range(K):\n",
        "            comp_positions = [p for p in positions if comp_assignments_ex0[p] == comp_id]\n",
        "            if comp_positions:\n",
        "                comp_mses = [mse_by_pos[p] for p in comp_positions]\n",
        "                axes[1].scatter(comp_positions, comp_mses, \n",
        "                               c=[colors[comp_id]], label=f'Component {comp_id}', \n",
        "                               s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
        "        \n",
        "        axes[1].plot(positions, mse_by_pos, 'k--', alpha=0.3, linewidth=1, label='Overall')\n",
        "        axes[1].set_xlabel(\"Position in Sequence\", fontsize=12)\n",
        "        axes[1].set_ylabel(\"Mean Squared Error\", fontsize=12)\n",
        "        axes[1].set_title('MSE by Position (Colored by Component)', fontsize=13)\n",
        "        axes[1].legend(loc='best', fontsize=10)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add boundary line\n",
        "        if context_length < len(positions):\n",
        "            axes[1].axvline(x=context_length-0.5, color='r', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nComponent usage statistics:\")\n",
        "    for comp_id in range(K):\n",
        "        comp_count = (data_sampler.component_assignments == comp_id).sum().item()\n",
        "        total_positions = data_sampler.component_assignments.numel()\n",
        "        pct = 100 * comp_count / total_positions\n",
        "        print(f\"  Component {comp_id}: used in {comp_count}/{total_positions} positions ({pct:.1f}%)\")\n",
        "        if comp_id in component_mse:\n",
        "            print(f\"    Average MSE: {component_mse[comp_id]:.6f}\")\n",
        "\n",
        "elif hasattr(task, 'get_mixture_info'):\n",
        "    # Legacy support for other mixture tasks\n",
        "    mix_info = task.get_mixture_info()\n",
        "    print(f\"Mixture info:\")\n",
        "    print(f\"  Number of components: {mix_info['n_components']}\")\n",
        "    print(f\"  Number of contexts: {mix_info['n_contexts']}\")\n",
        "    print(f\"  Context assignments: {mix_info['context_assignments']}\")\n",
        "    print(f\"  Target assignment: {mix_info['target_assignment']}\")\n",
        "    \n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for comp_id in range(mix_info['n_components']):\n",
        "        comp_mask = mix_info['target_assignment'] == comp_id\n",
        "        if comp_mask.any():\n",
        "            comp_loss = loss[comp_mask].mean(axis=0)\n",
        "            \n",
        "            plt.plot([0], comp_loss, marker='o', linestyle='', markersize=10, \n",
        "                     label=f'Component {comp_id} (MSE: {comp_loss[0]:.4f})')\n",
        "\n",
        "    plt.xticks([0], [f\"Position {predict_inds[0] if predict_inds else 0}\"])\n",
        "    plt.xlabel(\"Prediction position\")\n",
        "    plt.ylabel(\"Squared error\")\n",
        "    plt.legend()\n",
        "    plt.title('Error by Mixture Component (Single Point)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be54bc7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/eddieqiao/Documents/mit/fall-2025/urop/in-context-learning/src/eval.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(state_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"DETAILED ERROR ANALYSIS BY POSITION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Compute squared errors\n",
        "if predict_inds is not None and len(predict_inds) > 0:\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        # All positions: pred and ys have same shape (B, T)\n",
        "        e = (pred - ys)**2  # (B, T)\n",
        "    else:\n",
        "        # Specific positions\n",
        "        e = (pred - ys[:, predict_inds])**2\n",
        "else:\n",
        "    e = (pred - ys)**2\n",
        "\n",
        "mse_by_pos = e.mean(dim=0)  # Average across batch\n",
        "print(f\"MSE by position: {mse_by_pos.tolist()}\")\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: MSE by position (line plot)\n",
        "if len(mse_by_pos.shape) == 0:\n",
        "    # Single value\n",
        "    axes[0, 0].bar([0], [mse_by_pos.item()])\n",
        "    axes[0, 0].set_xlabel(\"Position\")\n",
        "    axes[0, 0].set_ylabel(\"MSE\")\n",
        "    axes[0, 0].set_title(f'MSE at Position (Value: {mse_by_pos.item():.6f})')\n",
        "else:\n",
        "    positions = range(len(mse_by_pos))\n",
        "    axes[0, 0].plot(positions, mse_by_pos.cpu().numpy(), 'o-', linewidth=2, markersize=5)\n",
        "    axes[0, 0].set_xlabel(\"Position in Sequence\", fontsize=11)\n",
        "    axes[0, 0].set_ylabel(\"Mean Squared Error\", fontsize=11)\n",
        "    axes[0, 0].set_title(f'MSE by Position (Mean: {mse_by_pos.mean():.6f})', fontsize=12)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add boundary line for group_mixture_linear\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < len(positions):\n",
        "            axes[0, 0].axvline(x=context_length-0.5, color='r', linestyle='--', alpha=0.5)\n",
        "            axes[0, 0].text(context_length, mse_by_pos.max() * 0.9, 'Context/Target\\nboundary', \n",
        "                           ha='center', fontsize=9, color='r')\n",
        "\n",
        "# Plot 2: Error distribution histogram\n",
        "axes[0, 1].hist(loss.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].set_xlabel(\"Squared Error\", fontsize=11)\n",
        "axes[0, 1].set_ylabel(\"Frequency\", fontsize=11)\n",
        "axes[0, 1].set_title(f'Error Distribution (Mean: {loss.mean():.4f})', fontsize=12)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Heatmap of errors by example and position (if multiple positions)\n",
        "if len(e.shape) == 2 and e.shape[1] > 1:\n",
        "    # Show first 20 examples to avoid overcrowding\n",
        "    n_examples_show = min(20, e.shape[0])\n",
        "    im = axes[1, 0].imshow(e[:n_examples_show].cpu().numpy(), aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
        "    axes[1, 0].set_xlabel(\"Position in Sequence\", fontsize=11)\n",
        "    axes[1, 0].set_ylabel(\"Example Index\", fontsize=11)\n",
        "    axes[1, 0].set_title(f'Squared Error Heatmap (First {n_examples_show} Examples)', fontsize=12)\n",
        "    plt.colorbar(im, ax=axes[1, 0], label='Squared Error')\n",
        "    \n",
        "    # Add boundary line\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < e.shape[1]:\n",
        "            axes[1, 0].axvline(x=context_length-0.5, color='cyan', linestyle='--', linewidth=2, alpha=0.7)\n",
        "else:\n",
        "    axes[1, 0].axis('off')\n",
        "\n",
        "# Plot 4: Box plot of errors by position (if multiple positions)\n",
        "if len(mse_by_pos.shape) > 0 and len(mse_by_pos) > 1:\n",
        "    # Group positions for box plot (every 5 positions or so)\n",
        "    n_positions = len(mse_by_pos)\n",
        "    if n_positions > 20:\n",
        "        # Sample positions for readability\n",
        "        step = max(1, n_positions // 20)\n",
        "        sampled_positions = list(range(0, n_positions, step))\n",
        "        sampled_errors = [e[:, pos].cpu().numpy() for pos in sampled_positions]\n",
        "        axes[1, 1].boxplot(sampled_errors, labels=[f'P{p}' for p in sampled_positions])\n",
        "        axes[1, 1].set_xlabel(\"Position (sampled)\", fontsize=11)\n",
        "    else:\n",
        "        errors_by_pos = [e[:, pos].cpu().numpy() for pos in range(n_positions)]\n",
        "        axes[1, 1].boxplot(errors_by_pos, labels=[f'P{p}' for p in range(n_positions)])\n",
        "        axes[1, 1].set_xlabel(\"Position\", fontsize=11)\n",
        "    axes[1, 1].set_ylabel(\"Squared Error\", fontsize=11)\n",
        "    axes[1, 1].set_title('Error Distribution by Position', fontsize=12)\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOverall statistics:\")\n",
        "if len(mse_by_pos.shape) == 0:\n",
        "    print(f\"  MSE: {mse_by_pos.item():.6f}\")\n",
        "else:\n",
        "    print(f\"  Mean MSE across all positions: {mse_by_pos.mean():.6f}\")\n",
        "    print(f\"  Std MSE across positions: {mse_by_pos.std():.6f}\")\n",
        "    print(f\"  Min MSE (best position): {mse_by_pos.min():.6f} at position {mse_by_pos.argmin().item()}\")\n",
        "    print(f\"  Max MSE (worst position): {mse_by_pos.max():.6f} at position {mse_by_pos.argmax().item()}\")\n",
        "    \n",
        "    # Show performance in context vs target regions\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < len(mse_by_pos):\n",
        "            context_mse = mse_by_pos[:context_length].mean()\n",
        "            target_mse = mse_by_pos[context_length:].mean()\n",
        "            print(f\"\\nPerformance by region:\")\n",
        "            print(f\"  Context clusters (0-{context_length-1}): Mean MSE = {context_mse:.6f}\")\n",
        "            print(f\"  Target cluster ({context_length}-{len(mse_by_pos)-1}): Mean MSE = {target_mse:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5fd9ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE VISUALIZATION: Performance at Every Index\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PERFORMANCE AT EVERY INDEX - DETAILED VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Compute errors for all positions\n",
        "if len(predict_inds) == ys.shape[1]:\n",
        "    # All positions predicted\n",
        "    e = (pred - ys)**2  # (B, T)\n",
        "    positions = list(range(ys.shape[1]))\n",
        "else:\n",
        "    # Specific positions\n",
        "    e = (pred - ys[:, predict_inds])**2\n",
        "    positions = predict_inds\n",
        "\n",
        "mse_by_pos = e.mean(dim=0).cpu().numpy()  # (T,)\n",
        "std_by_pos = e.std(dim=0).cpu().numpy()   # (T,)\n",
        "\n",
        "# Create comprehensive figure\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Plot 1: MSE by position with error bars (top left)\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "ax1.errorbar(positions, mse_by_pos, yerr=std_by_pos, \n",
        "            fmt='o-', linewidth=2, markersize=5, capsize=3, capthick=1.5)\n",
        "ax1.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax1.set_ylabel(\"Mean Squared Error\", fontsize=11)\n",
        "ax1.set_title('MSE by Position (with Std Dev)', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add context/target boundary if applicable\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax1.axvline(x=context_length-0.5, color='r', linestyle='--', linewidth=2, alpha=0.6)\n",
        "        ax1.text(context_length, mse_by_pos.max() * 0.95, 'Context/Target\\nBoundary', \n",
        "                ha='center', fontsize=9, color='r', fontweight='bold')\n",
        "\n",
        "# Plot 2: Log-scale MSE (top middle)\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "ax2.semilogy(positions, mse_by_pos, 'o-', linewidth=2, markersize=5)\n",
        "ax2.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax2.set_ylabel(\"MSE (log scale)\", fontsize=11)\n",
        "ax2.set_title('MSE by Position (Log Scale)', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, which='both')\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax2.axvline(x=context_length-0.5, color='r', linestyle='--', linewidth=2, alpha=0.6)\n",
        "\n",
        "# Plot 3: Relative error (MSE normalized by mean) (top right)\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "mean_mse = mse_by_pos.mean()\n",
        "relative_error = mse_by_pos / mean_mse\n",
        "ax3.plot(positions, relative_error, 'o-', linewidth=2, markersize=5, color='green')\n",
        "ax3.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Mean')\n",
        "ax3.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax3.set_ylabel(\"Relative MSE (vs Mean)\", fontsize=11)\n",
        "ax3.set_title('Relative Performance by Position', fontsize=12, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax3.axvline(x=context_length-0.5, color='r', linestyle='--', linewidth=2, alpha=0.6)\n",
        "\n",
        "# Plot 4: Error heatmap (bottom left, larger)\n",
        "ax4 = plt.subplot(2, 3, (4, 5))\n",
        "n_examples_show = min(30, e.shape[0])\n",
        "im = ax4.imshow(e[:n_examples_show].cpu().numpy(), aspect='auto', \n",
        "                cmap='YlOrRd', interpolation='nearest', vmin=0, vmax=e.max().item())\n",
        "ax4.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax4.set_ylabel(\"Example Index\", fontsize=11)\n",
        "ax4.set_title(f'Error Heatmap (First {n_examples_show} Examples)', fontsize=12, fontweight='bold')\n",
        "cbar = plt.colorbar(im, ax=ax4)\n",
        "cbar.set_label('Squared Error', fontsize=10)\n",
        "\n",
        "# Add boundary line\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax4.axvline(x=positions.index(context_length) if context_length in positions else context_length-0.5, \n",
        "                   color='cyan', linestyle='--', linewidth=2, alpha=0.8)\n",
        "\n",
        "# Plot 5: Box plot by position groups (bottom right)\n",
        "ax5 = plt.subplot(2, 3, 6)\n",
        "if len(positions) > 10:\n",
        "    # Group positions into bins\n",
        "    n_bins = min(10, len(positions))\n",
        "    bin_size = len(positions) // n_bins\n",
        "    position_groups = []\n",
        "    group_labels = []\n",
        "    for i in range(n_bins):\n",
        "        start_idx = i * bin_size\n",
        "        end_idx = (i + 1) * bin_size if i < n_bins - 1 else len(positions)\n",
        "        group_positions = positions[start_idx:end_idx]\n",
        "        group_errors = [e[:, pos].cpu().numpy() for pos in group_positions]\n",
        "        position_groups.append(group_errors)\n",
        "        group_labels.append(f'{start_idx}-{end_idx-1}')\n",
        "    \n",
        "    # Flatten for box plot\n",
        "    box_data = []\n",
        "    box_labels = []\n",
        "    for group_errors, label in zip(position_groups, group_labels):\n",
        "        for pos_errors in group_errors:\n",
        "            box_data.append(pos_errors)\n",
        "            box_labels.append(label)\n",
        "    \n",
        "    bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "else:\n",
        "    # Show all positions\n",
        "    box_data = [e[:, pos].cpu().numpy() for pos in positions]\n",
        "    box_labels = [f'P{p}' for p in positions]\n",
        "    bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "ax5.set_xlabel(\"Position Range\", fontsize=11)\n",
        "ax5.set_ylabel(\"Squared Error\", fontsize=11)\n",
        "ax5.set_title('Error Distribution by Position Group', fontsize=12, fontweight='bold')\n",
        "ax5.tick_params(axis='x', rotation=45)\n",
        "ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total positions evaluated: {len(positions)}\")\n",
        "print(f\"Mean MSE across all positions: {mse_by_pos.mean():.6f}\")\n",
        "print(f\"Std MSE across positions: {mse_by_pos.std():.6f}\")\n",
        "print(f\"Min MSE: {mse_by_pos.min():.6f} at position {positions[np.argmin(mse_by_pos)]}\")\n",
        "print(f\"Max MSE: {mse_by_pos.max():.6f} at position {positions[np.argmax(mse_by_pos)]}\")\n",
        "print(f\"Median MSE: {np.median(mse_by_pos):.6f}\")\n",
        "\n",
        "# Performance by region (if applicable)\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        context_positions = [p for p in positions if p < context_length]\n",
        "        target_positions = [p for p in positions if p >= context_length]\n",
        "        \n",
        "        if context_positions:\n",
        "            context_indices = [positions.index(p) for p in context_positions]\n",
        "            context_mse = mse_by_pos[context_indices].mean()\n",
        "            print(f\"\\nContext Clusters (positions 0-{context_length-1}):\")\n",
        "            print(f\"  Mean MSE: {context_mse:.6f}\")\n",
        "            print(f\"  Positions: {len(context_positions)}\")\n",
        "        \n",
        "        if target_positions:\n",
        "            target_indices = [positions.index(p) for p in target_positions]\n",
        "            target_mse = mse_by_pos[target_indices].mean()\n",
        "            print(f\"\\nTarget Cluster (positions {context_length}-{len(positions)-1}):\")\n",
        "            print(f\"  Mean MSE: {target_mse:.6f}\")\n",
        "            print(f\"  Positions: {len(target_positions)}\")\n",
        "            print(f\"  Ratio (target/context): {target_mse/context_mse:.3f}\" if context_positions else \"\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea393db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs stats: mean=-0.0017, std=0.5688, min=-3.4024, max=3.4349\n",
            "ys stats: mean=-0.0052, std=0.1817, min=-0.8882, max=0.7165\n",
            "coefficients[0]: tensor([ 0.0216,  0.0845, -0.0595,  0.0055, -0.0728,  0.1485,  0.0060, -0.0074,\n",
            "        -0.0203,  0.0035])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.7556, -0.0793,  0.7708, -0.8922, -0.4725,  0.5542, -1.1830,  0.5065,\n",
              "           0.2305, -0.2648],\n",
              "         [-0.2118, -0.7556, -0.0793,  0.7708, -0.8922, -0.4725,  0.5542, -1.1830,\n",
              "           0.5065,  0.2305],\n",
              "         [ 0.1393, -0.2118, -0.7556, -0.0793,  0.7708, -0.8922, -0.4725,  0.5542,\n",
              "          -1.1830,  0.5065],\n",
              "         [-0.3126,  0.1393, -0.2118, -0.7556, -0.0793,  0.7708, -0.8922, -0.4725,\n",
              "           0.5542, -1.1830],\n",
              "         [ 0.1181, -0.3126,  0.1393, -0.2118, -0.7556, -0.0793,  0.7708, -0.8922,\n",
              "          -0.4725,  0.5542],\n",
              "         [ 0.3356,  0.1181, -0.3126,  0.1393, -0.2118, -0.7556, -0.0793,  0.7708,\n",
              "          -0.8922, -0.4725],\n",
              "         [-0.2420,  0.3356,  0.1181, -0.3126,  0.1393, -0.2118, -0.7556, -0.0793,\n",
              "           0.7708, -0.8922],\n",
              "         [-0.1926, -0.2420,  0.3356,  0.1181, -0.3126,  0.1393, -0.2118, -0.7556,\n",
              "          -0.0793,  0.7708],\n",
              "         [-0.0583, -0.1926, -0.2420,  0.3356,  0.1181, -0.3126,  0.1393, -0.2118,\n",
              "          -0.7556, -0.0793],\n",
              "         [-0.0018, -0.0583, -0.1926, -0.2420,  0.3356,  0.1181, -0.3126,  0.1393,\n",
              "          -0.2118, -0.7556],\n",
              "         [ 0.0166, -0.0018, -0.0583, -0.1926, -0.2420,  0.3356,  0.1181, -0.3126,\n",
              "           0.1393, -0.2118],\n",
              "         [-0.0088,  0.0166, -0.0018, -0.0583, -0.1926, -0.2420,  0.3356,  0.1181,\n",
              "          -0.3126,  0.1393],\n",
              "         [ 0.2592, -0.0088,  0.0166, -0.0018, -0.0583, -0.1926, -0.2420,  0.3356,\n",
              "           0.1181, -0.3126],\n",
              "         [ 0.2235,  0.2592, -0.0088,  0.0166, -0.0018, -0.0583, -0.1926, -0.2420,\n",
              "           0.3356,  0.1181],\n",
              "         [-0.0321,  0.2235,  0.2592, -0.0088,  0.0166, -0.0018, -0.0583, -0.1926,\n",
              "          -0.2420,  0.3356],\n",
              "         [ 0.1221, -0.0321,  0.2235,  0.2592, -0.0088,  0.0166, -0.0018, -0.0583,\n",
              "          -0.1926, -0.2420],\n",
              "         [-0.2547,  0.1221, -0.0321,  0.2235,  0.2592, -0.0088,  0.0166, -0.0018,\n",
              "          -0.0583, -0.1926],\n",
              "         [-0.0416, -0.2547,  0.1221, -0.0321,  0.2235,  0.2592, -0.0088,  0.0166,\n",
              "          -0.0018, -0.0583],\n",
              "         [-0.0500, -0.0416, -0.2547,  0.1221, -0.0321,  0.2235,  0.2592, -0.0088,\n",
              "           0.0166, -0.0018],\n",
              "         [ 0.1471, -0.0500, -0.0416, -0.2547,  0.1221, -0.0321,  0.2235,  0.2592,\n",
              "          -0.0088,  0.0166]]),\n",
              " tensor([ 0.0264, -0.0621, -0.1401,  0.1163,  0.0327, -0.0498, -0.0500,  0.0081,\n",
              "         -0.0388, -0.0030,  0.0695, -0.0129, -0.0279,  0.0130,  0.0084, -0.0053,\n",
              "         -0.0116, -0.0080,  0.0480, -0.0140]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"MODEL CAPABILITY ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if conf.training.task == \"group_mixture_linear\":\n",
        "    print(\"On-the-fly Mixture Linear Regression Analysis:\")\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    T_target = data_sampler.target_cluster_context_points\n",
        "    context_length = K * C\n",
        "    total_length = sequence_structure['total_length'] if sequence_structure else n_points\n",
        "    \n",
        "    print(f\"  - Task structure:\")\n",
        "    print(f\"    * {K} mixture components\")\n",
        "    print(f\"    * {K} context clusters, each with {C} points\")\n",
        "    print(f\"    * 1 target cluster with {T_target} context points + 1 prediction point\")\n",
        "    print(f\"    * Total sequence length: {total_length}\")\n",
        "    print(f\"  - Model must:\")\n",
        "    print(f\"    1. Learn {K} weight vectors from context clusters (positions 0-{context_length-1})\")\n",
        "    print(f\"    2. Infer which component is used in target cluster from first {T_target} target points\")\n",
        "    print(f\"    3. Predict ALL {total_length} positions using the learned components\")\n",
        "    print(f\"  - Clusters are in fixed order (no randomization)\")\n",
        "    print(f\"  - All components guaranteed to appear in context clusters\")\n",
        "    \n",
        "    # Analyze component usage\n",
        "    if hasattr(data_sampler, 'component_assignments'):\n",
        "        print(f\"\\n  - Component usage in this batch:\")\n",
        "        for comp_id in range(K):\n",
        "            comp_count = (data_sampler.component_assignments == comp_id).sum().item()\n",
        "            print(f\"    Component {comp_id}: {comp_count} positions\")\n",
        "        \n",
        "        # Check if target components appeared in context\n",
        "        if hasattr(data_sampler, 'target_components'):\n",
        "            target_in_context = 0\n",
        "            for b in range(batch_size):\n",
        "                target_comp = data_sampler.target_components[b].item()\n",
        "                context_comps = set(data_sampler.component_assignments[b, :context_length].cpu().tolist())\n",
        "                if target_comp in context_comps:\n",
        "                    target_in_context += 1\n",
        "            print(f\"  - Target component appeared in context for {target_in_context}/{batch_size} examples\")\n",
        "\n",
        "elif predict_inds is not None and sequence_structure is not None:\n",
        "    print(\"Multi-context learning analysis:\")\n",
        "    if 'n_contexts' in sequence_structure:\n",
        "        print(f\"  - Model sees {sequence_structure['n_contexts']} context series\")\n",
        "        print(f\"  - Each context has {sequence_structure['context_length']} points\") \n",
        "        print(f\"  - Predicts {sequence_structure['predict_length']} future points\")\n",
        "    else:\n",
        "        print(f\"  - Sequence structure: {sequence_structure}\")\n",
        "        print(f\"  - Predicting {len(predict_inds)} positions\")\n",
        "    \n",
        "    if hasattr(task, 'get_mixture_info'):\n",
        "        mix_info = task.get_mixture_info()\n",
        "        print(f\"  - Must identify correct component from {mix_info['n_components']} possibilities\")\n",
        "        \n",
        "        correct_identification = 0\n",
        "        total = 0\n",
        "        \n",
        "        for b in range(batch_size):\n",
        "            target_comp = mix_info['target_assignment'][b]\n",
        "            context_comps = mix_info['context_assignments'][b]\n",
        "            \n",
        "            if target_comp in context_comps:\n",
        "                total += 1\n",
        "                \n",
        "        if total > 0:\n",
        "            print(f\"  - Target component appeared in context for {total}/{batch_size} examples\")\n",
        "\n",
        "else:\n",
        "    print(\"Standard in-context learning analysis:\")\n",
        "    print(f\"  - Model learns from increasing context (up to {n_points} points)\")\n",
        "    print(f\"  - Task: {conf.training.task}\")\n",
        "    print(f\"  - Data: {conf.training.data}\")\n",
        "    if predict_inds is not None:\n",
        "        print(f\"  - Predicting {len(predict_inds)} positions\")\n",
        "    else:\n",
        "        print(f\"  - Predicting all positions (autoregressive)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394cc45d",
      "metadata": {},
      "source": [
        "# ✅ FIX APPLIED\n",
        "\n",
        "## The Bug and the Fix\n",
        "\n",
        "**The Bug:** The targets (ys) were **noiseless predictions** instead of actual noisy values.\n",
        "- Before: `ys = task.evaluate(xs)` computed ys as dot products (no noise!)\n",
        "- This allowed MSE < noise_variance (0.04) because targets had no noise\n",
        "\n",
        "**The Fix:**\n",
        "1. **Modified `ARWarmupSampler.sample_xs()`** to store actual noisy next values in `self.current_ys`\n",
        "2. **Modified training loop** to use `data_sampler.current_ys` instead of `task.evaluate(xs)` for AR tasks\n",
        "3. **Need to update eval cell 6** - replace `ys = task.evaluate(xs)` with:\n",
        "   ```python\n",
        "   if hasattr(data_sampler, 'current_ys'):\n",
        "       ys = data_sampler.current_ys\n",
        "   else:\n",
        "       ys = task.evaluate(xs)\n",
        "   ```\n",
        "\n",
        "**After the fix:**\n",
        "- ys will contain the actual noisy values from the AR sequence\n",
        "- Theoretical minimum MSE will correctly be 0.04 (noise variance)\n",
        "- The model will be learning to predict noisy targets, which is the correct formulation\n",
        "\n",
        "**To apply:** \n",
        "1. Restart the kernel\n",
        "2. Re-run cells from the beginning\n",
        "3. You should now see MSE values that make sense relative to the noise floor!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae10a8c2",
      "metadata": {},
      "source": [
        "# Manual Update Required for Cell 6\n",
        "\n",
        "**Before running the evaluation, update the cell where `ys = task.evaluate(xs)` appears:**\n",
        "\n",
        "### Find this code (around cell 5 or 6):\n",
        "```python\n",
        "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
        "\n",
        "task_sampler_args = {\"coefficients\": data_sampler.current_coefficients}\n",
        "task = task_sampler(**task_sampler_args)\n",
        "ys = task.evaluate(xs)  # ← OLD (noiseless)\n",
        "```\n",
        "\n",
        "### Replace with:\n",
        "```python\n",
        "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
        "\n",
        "task_sampler_args = {\"coefficients\": data_sampler.current_coefficients}\n",
        "task = task_sampler(**task_sampler_args)\n",
        "\n",
        "# Use actual noisy targets (includes noise from AR generation)\n",
        "if hasattr(data_sampler, 'current_ys'):\n",
        "    ys = data_sampler.current_ys\n",
        "    print(\"✓ Using actual noisy targets\")\n",
        "else:\n",
        "    ys = task.evaluate(xs)\n",
        "    print(\"⚠️ Falling back to noiseless predictions\")\n",
        "```\n",
        "\n",
        "After this change, **restart the kernel and re-run from the beginning!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "in-context-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
