{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c640866",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package.split('==')[0].split('>=')[0].split('<=')[0]\n",
        "    \n",
        "    # Special handling for protobuf to avoid version conflicts\n",
        "    if import_name == \"wandb\":\n",
        "        try:\n",
        "            # First ensure protobuf is at compatible version\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf<=3.20.3\", \"--quiet\", \"--user\"])\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"✓ {package} already installed\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\", \"--user\"])\n",
        "            print(f\"✓ Installed {package}\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Failed to install {package}: {e}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error importing {import_name}: {e}\")\n",
        "        print(f\"  Attempting to reinstall...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--force-reinstall\", \"--quiet\", \"--user\"])\n",
        "            print(f\"✓ Reinstalled {package}\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Failed to reinstall {package}: {e}\")\n",
        "            return False\n",
        "\n",
        "# First, ensure protobuf is at a compatible version to avoid wandb issues\n",
        "print(\"Setting up temporary environment...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Ensuring compatible protobuf version...\")\n",
        "\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf<=3.20.3\", \"--quiet\", \"--user\"])\n",
        "    print(\"✓ Set protobuf to compatible version\")\n",
        "except:\n",
        "    print(\"Note: Could not adjust protobuf version, continuing...\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Install required packages with specific versions to avoid conflicts\n",
        "packages = [\n",
        "    (\"protobuf<=3.20.3\", \"google.protobuf\"),  # Force compatible version first\n",
        "    (\"torch\", \"torch\"),\n",
        "    (\"numpy==1.22.3\", \"numpy\"),\n",
        "    (\"pandas==1.4.2\", \"pandas\"),\n",
        "    (\"matplotlib==3.5.2\", \"matplotlib\"),\n",
        "    (\"seaborn==0.11.2\", \"seaborn\"),\n",
        "    (\"tqdm==4.64.0\", \"tqdm\"),\n",
        "    (\"munch==2.5.0\", \"munch\"),\n",
        "    (\"pyyaml==6.0\", \"yaml\"),\n",
        "    (\"transformers==4.17.0\", \"transformers\"),\n",
        "    (\"wandb==0.12.11\", \"wandb\"),\n",
        "    (\"scikit-learn==1.0.2\", \"sklearn\"),\n",
        "]\n",
        "\n",
        "# Install packages in order\n",
        "successful_installs = []\n",
        "for package, import_name in packages:\n",
        "    if install_if_missing(package, import_name):\n",
        "        successful_installs.append(import_name)\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Try a workaround for the protobuf issue\n",
        "print(\"Attempting to fix protobuf descriptor issue...\")\n",
        "try:\n",
        "    # Clear any cached modules\n",
        "    for module in list(sys.modules.keys()):\n",
        "        if 'google.protobuf' in module or 'wandb' in module:\n",
        "            del sys.modules[module]\n",
        "    \n",
        "    # Set environment variable as suggested in error message\n",
        "    os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "    print(\"✓ Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not apply workaround: {e}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Set up paths\n",
        "USER_HOME = os.path.expanduser(\"~\")\n",
        "PROJECT_DIR = os.path.join(USER_HOME, \"icl-time-series\")\n",
        "SRC_DIR = os.path.join(PROJECT_DIR, \"src\")\n",
        "\n",
        "# Add src directory to path\n",
        "if SRC_DIR not in sys.path:\n",
        "    sys.path.insert(0, SRC_DIR)\n",
        "    print(f\"✓ Added src directory: {SRC_DIR}\")\n",
        "else:\n",
        "    print(f\"✓ src directory already in path\")\n",
        "\n",
        "# Verify imports with error handling\n",
        "print(\"\\nVerifying imports...\")\n",
        "required_imports = {\n",
        "    'torch': 'torch',\n",
        "    'numpy': 'np',\n",
        "    'pandas': 'pd',\n",
        "    'matplotlib.pyplot': 'plt',\n",
        "    'seaborn': 'sns',\n",
        "    'munch': 'Munch',\n",
        "    'yaml': 'yaml',\n",
        "    'tqdm': 'tqdm',\n",
        "    'sklearn': 'sklearn'\n",
        "}\n",
        "\n",
        "successful_imports = []\n",
        "for module, alias in required_imports.items():\n",
        "    try:\n",
        "        if '.' in module:\n",
        "            # Handle submodules like matplotlib.pyplot\n",
        "            parts = module.split('.')\n",
        "            base_module = __import__(parts[0])\n",
        "            for part in parts[1:]:\n",
        "                base_module = getattr(base_module, part)\n",
        "            globals()[alias] = base_module\n",
        "        else:\n",
        "            globals()[alias] = __import__(module)\n",
        "        successful_imports.append(module)\n",
        "        print(f\"✓ {module} imported successfully\")\n",
        "    except ImportError as e:\n",
        "        print(f\"✗ Failed to import {module}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Unexpected error importing {module}: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Setup Summary:\")\n",
        "print(f\"- Attempted to install: {len(packages)} packages\")\n",
        "print(f\"- Successfully imported: {len(successful_imports)}/{len(required_imports)} core modules\")\n",
        "print(\"\\nIf you continue to have wandb/protobuf issues:\")\n",
        "print(\"1. Restart the kernel and run only this cell\")\n",
        "print(\"2. Or try: pip install wandb==0.12.11 protobuf==3.20.3\")\n",
        "print(\"3. Or downgrade: pip install protobuf==3.20.3\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2702f0fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the user's home directory (works for ORCD cluster)\n",
        "USER_HOME = os.path.expanduser(\"~\")\n",
        "# Project directory: ~/icl-time-series\n",
        "PROJECT_DIR = os.path.join(USER_HOME, \"icl-time-series\")\n",
        "SRC_DIR = os.path.join(PROJECT_DIR, \"src\")\n",
        "\n",
        "# Add src directory to path\n",
        "if SRC_DIR not in sys.path:\n",
        "    sys.path.insert(0, SRC_DIR)\n",
        "    print(f\"✓ Added src directory to path: {SRC_DIR}\")\n",
        "else:\n",
        "    print(f\"✓ src directory already in path: {SRC_DIR}\")\n",
        "\n",
        "# Check if we're in a conda environment\n",
        "CONDA_PREFIX = os.environ.get('CONDA_PREFIX')\n",
        "if CONDA_PREFIX:\n",
        "    print(f\"✓ Running in conda environment: {CONDA_PREFIX}\")\n",
        "    print(f\"  Python version: {sys.version}\")\n",
        "    print(f\"  Python executable: {sys.executable}\")\n",
        "else:\n",
        "    print(\"⚠ Warning: Not in a conda environment. Make sure to activate 'in-context-learning' first.\")\n",
        "    print(f\"  Current Python: {sys.executable}\")\n",
        "\n",
        "# Verify we can import project modules\n",
        "try:\n",
        "    import eval\n",
        "    print(\"✓ Successfully imported eval module\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import eval module: {e}\")\n",
        "    print(f\"  Make sure you're in the conda environment and src/ directory is correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6cfeb1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/eddieqiao/miniconda3/envs/in-context-learning/lib/python3.11/site-packages/munch/__init__.py:24: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "import re\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
        "from plot_utils import basic_plot, collect_results, relevant_model_names\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "sns.set_theme('notebook', 'darkgrid')\n",
        "palette = sns.color_palette('colorblind')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9980951",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Update these to match your training run\n",
        "USER_HOME = os.path.expanduser(\"~\")\n",
        "PROJECT_DIR = os.path.join(USER_HOME, \"icl-time-series\")\n",
        "\n",
        "# Model output directory (relative to project root)\n",
        "run_dir = os.path.join(USER_HOME, \"models\", \"group_mixture_linear\")\n",
        "# Alternative: if models are stored elsewhere, use absolute path:\n",
        "# run_dir = os.path.join(USER_HOME, \"models\", \"group_mixture_linear\")\n",
        "\n",
        "task = \"group_mixture_linear\"  # Update to match your task\n",
        "run_id = \"0092300d-259b-46b0-bfa7-2117e360f05d\"  # Replace with actual run_id from training output\n",
        "\n",
        "run_path = os.path.join(run_dir, run_id)\n",
        "recompute_metrics = False\n",
        "\n",
        "print(f\"Run directory: {run_dir}\")\n",
        "print(f\"Run path: {run_path}\")\n",
        "print(f\"Task: {task}\")\n",
        "\n",
        "if recompute_metrics:\n",
        "    get_run_metrics(run_path)  # these are normally precomputed at the end of training\n",
        "else:\n",
        "    print(\"Using precomputed metrics (set recompute_metrics=True to recompute)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71dc5b5",
      "metadata": {},
      "source": [
        "# Interactive setup\n",
        "\n",
        "We will now directly load the model and measure its in-context learning ability on a batch of random inputs. (In the paper we average over multiple such batches to obtain better estimates.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e55bb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from samplers import get_data_sampler\n",
        "from tasks import get_task_sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87fcb390",
      "metadata": {},
      "outputs": [],
      "source": [
        "model, conf = get_model_from_run(run_path)\n",
        "n_dims = conf.model.n_dims\n",
        "batch_size = conf.training.batch_size\n",
        "\n",
        "\n",
        "# Convert task_kwargs properly\n",
        "task_kwargs = {}\n",
        "if hasattr(conf.training, 'task_kwargs') and conf.training.task_kwargs:\n",
        "    if hasattr(conf.training.task_kwargs, '__dict__'):\n",
        "        task_kwargs = conf.training.task_kwargs.__dict__\n",
        "    else:\n",
        "        task_kwargs = conf.training.task_kwargs\n",
        "\n",
        "print(\"Task kwargs:\", task_kwargs)\n",
        "\n",
        "# Override predict mode for eval: None = use run config; True = one position; False = all positions\n",
        "EVAL_PREDICT_TARGET_ONLY = None\n",
        "if EVAL_PREDICT_TARGET_ONLY is not None:\n",
        "    task_kwargs = dict(task_kwargs)\n",
        "    task_kwargs['predict_target_only'] = EVAL_PREDICT_TARGET_ONLY\n",
        "\n",
        "data_sampler = get_data_sampler(conf.training.data, n_dims=n_dims, **task_kwargs)\n",
        "\n",
        "task_sampler = get_task_sampler(\n",
        "    conf.training.task,\n",
        "    n_dims,\n",
        "    batch_size,\n",
        "    num_tasks=conf.training.num_tasks if hasattr(conf.training, 'num_tasks') else None,\n",
        "    **task_kwargs,\n",
        ")\n",
        "\n",
        "sequence_structure = None\n",
        "predict_inds = None\n",
        "if hasattr(data_sampler, 'get_sequence_structure'):\n",
        "    sequence_structure = data_sampler.get_sequence_structure()\n",
        "    predict_inds = sequence_structure.get('predict_inds', [])\n",
        "    print(f\"Multi-context structure: {sequence_structure}\")\n",
        "    print(f\"Will predict {len(predict_inds)} indices: {predict_inds[:10]}...\" if len(predict_inds) > 10 else f\"Will predict indices: {predict_inds}\")\n",
        "    \n",
        "    # Predict mode: one position (target only) or all positions — from run config or EVAL_PREDICT_TARGET_ONLY\n",
        "    if conf.training.task == \"group_mixture_linear\":\n",
        "        print(f\"\\nTask: {conf.training.task}\")\n",
        "        print(f\"  - Total sequence length: {sequence_structure['total_length']}\")\n",
        "        mode_str = \"1 position (target only)\" if len(predict_inds) == 1 else f\"all {len(predict_inds)} positions\"\n",
        "        print(f\"  - Predicting {mode_str}\")\n",
        "        if hasattr(data_sampler, 'n_components'):\n",
        "            K = data_sampler.n_components\n",
        "            C = data_sampler.contexts_per_component\n",
        "            T_target = data_sampler.target_cluster_context_points\n",
        "            context_length = K * C\n",
        "            print(f\"  - Structure: {K} context clusters × {C} points + target cluster ({T_target} context + 1 prediction)\")\n",
        "            print(f\"  - Context clusters: positions 0-{context_length-1}\")\n",
        "            print(f\"  - Target cluster: positions {context_length}-{sequence_structure['total_length']-1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ea2a21",
      "metadata": {},
      "outputs": [],
      "source": [
        "if sequence_structure is not None:\n",
        "    n_points = sequence_structure['total_length']\n",
        "else:\n",
        "    n_points = conf.training.curriculum.points.end\n",
        "\n",
        "# For group_mixture_linear, sample_xs() must be called first to set current_components\n",
        "# and component_assignments; then we create the task and evaluate.\n",
        "task_sampler_args = {}\n",
        "if conf.training.task == \"group_mixture_linear\":\n",
        "    xs = data_sampler.sample_xs(b_size=batch_size, n_points=n_points)\n",
        "    assert hasattr(data_sampler, \"current_components\") and data_sampler.current_components is not None, \\\n",
        "        \"Sampler must set current_components in sample_xs()\"\n",
        "    assert hasattr(data_sampler, \"component_assignments\") and data_sampler.component_assignments is not None, \\\n",
        "        \"Sampler must set component_assignments in sample_xs()\"\n",
        "    task = task_sampler(\n",
        "        components=data_sampler.current_components,\n",
        "        component_assignments=data_sampler.component_assignments,\n",
        "        **task_sampler_args,\n",
        "    )\n",
        "    ys = task.evaluate(xs)\n",
        "else:\n",
        "    task = task_sampler(**task_sampler_args)\n",
        "    xs = data_sampler.sample_xs(b_size=batch_size, n_points=n_points)\n",
        "    ys = task.evaluate(xs)\n",
        "\n",
        "print(f\"Data shapes - xs: {xs.shape}, ys: {ys.shape}\")\n",
        "\n",
        "print(f\"xs stats: mean={xs.mean():.4f}, std={xs.std():.4f}\")\n",
        "print(f\"ys stats: mean={ys.mean():.4f}, std={ys.std():.4f}\")\n",
        "\n",
        "print(\"\\nFirst training example (first few points):\")\n",
        "print(f\"xs[0, :5]: {xs[0, :5]}\")\n",
        "print(f\"ys[0, :5]: {ys[0, :5]}\")\n",
        "\n",
        "# Show structure for group_mixture_linear\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'component_assignments'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    T_target = data_sampler.target_cluster_context_points\n",
        "    context_length = K * C\n",
        "    print(f\"\\nSequence structure (first example):\")\n",
        "    print(f\"  Component assignments: {data_sampler.component_assignments[0].cpu().tolist()}\")\n",
        "    print(f\"  Context clusters (0-{context_length-1}): components {data_sampler.component_assignments[0, :context_length].cpu().tolist()}\")\n",
        "    print(f\"  Target cluster ({context_length}-{n_points-1}): components {data_sampler.component_assignments[0, context_length:].cpu().tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbfc26b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/eddieqiao/miniconda3/envs/in-context-learning/lib/python3.11/site-packages/munch/__init__.py:24: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    if predict_inds is not None and len(predict_inds) > 0 and sequence_structure is not None:\n",
        "        # Check if predicting all positions (autoregressive mode)\n",
        "        if len(predict_inds) == ys.shape[1] and set(predict_inds) == set(range(ys.shape[1])):\n",
        "            # Predicting all positions: use standard autoregressive call\n",
        "            pred = model(xs, ys)\n",
        "            print(f\"Autoregressive prediction (all positions) shape: {pred.shape}\")\n",
        "        else:\n",
        "            # Predicting specific positions\n",
        "            pred = model(xs, ys, inds=predict_inds, sequence_structure=sequence_structure)\n",
        "            print(f\"Multi-context prediction shape: {pred.shape}\")\n",
        "    else:\n",
        "        pred = model(xs, ys)\n",
        "        print(f\"Standard prediction shape: {pred.shape}\")\n",
        "\n",
        "print(\"\\nPrediction examples:\")\n",
        "if predict_inds is not None and len(predict_inds) > 0:\n",
        "    # If predicting all positions, pred shape is (B, T)\n",
        "    # If predicting specific positions, pred shape is (B, len(predict_inds))\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        # All positions predicted\n",
        "        for i in range(min(3, batch_size)):\n",
        "            print(f\"Example {i} (showing first 10 positions):\")\n",
        "            print(f\"  Actual: {ys[i, :10].numpy()}\")\n",
        "            print(f\"  Pred:   {pred[i, :10].numpy()}\")\n",
        "            print(f\"  Error:  {(pred[i, :10] - ys[i, :10]).abs().numpy()}\")\n",
        "    else:\n",
        "        # Specific positions predicted\n",
        "        for i in range(min(3, batch_size)):\n",
        "            actual_targets = ys[i, predict_inds]\n",
        "            predictions = pred[i]\n",
        "            print(f\"Example {i}:\")\n",
        "            print(f\"  Actual: {actual_targets[:5].numpy()}\")\n",
        "            print(f\"  Pred:   {predictions[:5].numpy()}\")\n",
        "else:\n",
        "    for i in range(min(3, batch_size)):\n",
        "        print(f\"Example {i}: actual={ys[i, -1]:.4f}, pred={pred[i, -1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c44eb2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "metric = task.get_metric()\n",
        "\n",
        "# Compute loss: if predicting all positions, pred and ys have same shape\n",
        "if predict_inds is not None and len(predict_inds) > 0:\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        # Predicting all positions: pred shape is (B, T), compare with ys (B, T)\n",
        "        loss = metric(pred, ys).numpy()  # (B, T)\n",
        "        print(f\"Loss computed on all {len(predict_inds)} positions\")\n",
        "    else:\n",
        "        # Predicting specific positions: pred shape is (B, len(predict_inds))\n",
        "        loss = metric(pred, ys[:, predict_inds]).numpy()\n",
        "        print(f\"Loss computed on {len(predict_inds)} prediction positions\")\n",
        "else:\n",
        "    loss = metric(pred, ys).numpy()\n",
        "\n",
        "print(f\"Loss shape: {loss.shape}\") \n",
        "print(f\"Mean loss: {loss.mean():.4f}\")\n",
        "\n",
        "# Visualize performance across all positions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: MSE by position\n",
        "if len(loss.shape) == 2 and loss.shape[1] > 1:\n",
        "    # Loss is (B, T) - compute mean across batch for each position\n",
        "    mse_by_position = loss.mean(axis=0)  # (T,)\n",
        "    positions = range(len(mse_by_position))\n",
        "    \n",
        "    axes[0].plot(positions, mse_by_position, 'o-', linewidth=2, markersize=4)\n",
        "    axes[0].set_xlabel(\"Position in Sequence\", fontsize=12)\n",
        "    axes[0].set_ylabel(\"Mean Squared Error\", fontsize=12)\n",
        "    axes[0].set_title(f'MSE by Position (Mean: {mse_by_position.mean():.4f})', fontsize=13)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add vertical line to separate context clusters from target cluster\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < len(positions):\n",
        "            axes[0].axvline(x=context_length-0.5, color='r', linestyle='--', alpha=0.5, label='Context/Target boundary')\n",
        "            axes[0].legend()\n",
        "    \n",
        "    # Plot 2: Loss distribution histogram\n",
        "    axes[1].hist(loss.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1].set_xlabel(\"Squared Error\", fontsize=12)\n",
        "    axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
        "    axes[1].set_title(f'Error Distribution (Mean: {loss.mean():.4f})', fontsize=13)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    # Single position or aggregated loss\n",
        "    axes[0].hist(loss.flatten(), bins=20, alpha=0.7, edgecolor='black')\n",
        "    axes[0].set_xlabel(\"Squared Error\", fontsize=12)\n",
        "    axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
        "    axes[0].set_title(f'Loss Distribution (Mean MSE: {loss.mean():.4f})', fontsize=13)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca9661a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Component-specific analysis for group_mixture_linear\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'component_assignments'):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"COMPONENT-SPECIFIC ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    T_target = data_sampler.target_cluster_context_points\n",
        "    context_length = K * C\n",
        "    \n",
        "    # Compute errors\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        e = (pred - ys)**2  # (B, T)\n",
        "    else:\n",
        "        e = (pred - ys[:, predict_inds])**2\n",
        "    \n",
        "    # Analyze by component\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: MSE by component (averaged across all positions where that component is used)\n",
        "    component_mse = {}\n",
        "    for comp_id in range(K):\n",
        "        # Find all positions where this component is used\n",
        "        comp_mask = data_sampler.component_assignments == comp_id  # (B, T)\n",
        "        if comp_mask.any():\n",
        "            # Average error across all positions using this component\n",
        "            comp_errors = e[comp_mask].mean().item()\n",
        "            component_mse[comp_id] = comp_errors\n",
        "    \n",
        "    if component_mse:\n",
        "        comp_ids = list(component_mse.keys())\n",
        "        comp_mses = [component_mse[cid] for cid in comp_ids]\n",
        "        axes[0].bar(comp_ids, comp_mses, alpha=0.7, edgecolor='black')\n",
        "        axes[0].set_xlabel(\"Component ID\", fontsize=12)\n",
        "        axes[0].set_ylabel(\"Mean Squared Error\", fontsize=12)\n",
        "        axes[0].set_title(f'MSE by Component (Averaged Across All Positions)', fontsize=13)\n",
        "        axes[0].grid(True, alpha=0.3, axis='y')\n",
        "        for i, (cid, mse) in enumerate(zip(comp_ids, comp_mses)):\n",
        "            axes[0].text(cid, mse, f'{mse:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    # Plot 2: MSE by position, colored by component\n",
        "    if len(e.shape) == 2 and e.shape[1] > 1:\n",
        "        mse_by_pos = e.mean(dim=0).cpu().numpy()  # (T,)\n",
        "        positions = range(len(mse_by_pos))\n",
        "        \n",
        "        # Color points by which component is used (for first example)\n",
        "        comp_assignments_ex0 = data_sampler.component_assignments[0].cpu().numpy()\n",
        "        colors = plt.cm.tab10(range(K))\n",
        "        \n",
        "        for comp_id in range(K):\n",
        "            comp_positions = [p for p in positions if comp_assignments_ex0[p] == comp_id]\n",
        "            if comp_positions:\n",
        "                comp_mses = [mse_by_pos[p] for p in comp_positions]\n",
        "                axes[1].scatter(comp_positions, comp_mses, \n",
        "                               c=[colors[comp_id]], label=f'Component {comp_id}', \n",
        "                               s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
        "        \n",
        "        axes[1].plot(positions, mse_by_pos, 'k--', alpha=0.3, linewidth=1, label='Overall')\n",
        "        axes[1].set_xlabel(\"Position in Sequence\", fontsize=12)\n",
        "        axes[1].set_ylabel(\"Mean Squared Error\", fontsize=12)\n",
        "        axes[1].set_title('MSE by Position (Colored by Component)', fontsize=13)\n",
        "        axes[1].legend(loc='best', fontsize=10)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add boundary line\n",
        "        if context_length < len(positions):\n",
        "            axes[1].axvline(x=context_length-0.5, color='r', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nComponent usage statistics:\")\n",
        "    for comp_id in range(K):\n",
        "        comp_count = (data_sampler.component_assignments == comp_id).sum().item()\n",
        "        total_positions = data_sampler.component_assignments.numel()\n",
        "        pct = 100 * comp_count / total_positions\n",
        "        print(f\"  Component {comp_id}: used in {comp_count}/{total_positions} positions ({pct:.1f}%)\")\n",
        "        if comp_id in component_mse:\n",
        "            print(f\"    Average MSE: {component_mse[comp_id]:.6f}\")\n",
        "\n",
        "elif hasattr(task, 'get_mixture_info'):\n",
        "    # Legacy support for other mixture tasks\n",
        "    mix_info = task.get_mixture_info()\n",
        "    print(f\"Mixture info:\")\n",
        "    print(f\"  Number of components: {mix_info['n_components']}\")\n",
        "    print(f\"  Number of contexts: {mix_info['n_contexts']}\")\n",
        "    print(f\"  Context assignments: {mix_info['context_assignments']}\")\n",
        "    print(f\"  Target assignment: {mix_info['target_assignment']}\")\n",
        "    \n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for comp_id in range(mix_info['n_components']):\n",
        "        comp_mask = mix_info['target_assignment'] == comp_id\n",
        "        if comp_mask.any():\n",
        "            comp_loss = loss[comp_mask].mean(axis=0)\n",
        "            \n",
        "            plt.plot([0], comp_loss, marker='o', linestyle='', markersize=10, \n",
        "                     label=f'Component {comp_id} (MSE: {comp_loss[0]:.4f})')\n",
        "\n",
        "    plt.xticks([0], [f\"Position {predict_inds[0] if predict_inds else 0}\"])\n",
        "    plt.xlabel(\"Prediction position\")\n",
        "    plt.ylabel(\"Squared error\")\n",
        "    plt.legend()\n",
        "    plt.title('Error by Mixture Component (Single Point)')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb84948f",
      "metadata": {},
      "source": [
        "# Test: Context (component 0, component 1) → Target component 0 vs 1 (predict all)\n",
        "\n",
        "Compare loss when we **fix context clusters** to show component 0 then component 1, and **vary only the target cluster**: ask for component 0 in all targets vs component 1 in all targets. Uses **predicting-all** mode so we get predictions (and loss) at every position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c426322d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only for group_mixture_linear: fixed context [0,1], target 0 vs target 1, predict-all\n",
        "if conf.training.task == \"group_mixture_linear\":\n",
        "    # Sampler in predict-all mode for this test\n",
        "    test_task_kwargs = dict(task_kwargs)\n",
        "    test_task_kwargs[\"predict_target_only\"] = False\n",
        "    data_sampler_test = get_data_sampler(conf.training.data, n_dims=n_dims, **test_task_kwargs)\n",
        "    seq_struct = data_sampler_test.get_sequence_structure()\n",
        "    predict_inds_all = seq_struct[\"predict_inds\"]\n",
        "    n_pts = seq_struct[\"total_length\"]\n",
        "    K = data_sampler_test.n_components\n",
        "    C = data_sampler_test.contexts_per_component\n",
        "    context_length = K * C\n",
        "    metric_fn = task.get_metric()\n",
        "\n",
        "    results = {}\n",
        "    for target_comp in [0, 1]:\n",
        "        xs = data_sampler_test.sample_xs(\n",
        "            n_points=n_pts,\n",
        "            b_size=batch_size,\n",
        "            fixed_cluster_assignments=[0, 1],\n",
        "            fixed_target_component=target_comp,\n",
        "        )\n",
        "        assert hasattr(data_sampler_test, \"current_components\") and data_sampler_test.current_components is not None\n",
        "        task_inst = task_sampler(\n",
        "            components=data_sampler_test.current_components,\n",
        "            component_assignments=data_sampler_test.component_assignments,\n",
        "        )\n",
        "        ys = task_inst.evaluate(xs)\n",
        "        with torch.no_grad():\n",
        "            pred = model(xs, ys)\n",
        "        sq_err = (pred - ys).pow(2)\n",
        "        mse_all = sq_err.mean().item()\n",
        "        mse_by_pos = sq_err.mean(dim=0).cpu().numpy()\n",
        "        mse_context = sq_err[:, :context_length].mean().item()\n",
        "        mse_target_cluster = sq_err[:, context_length:].mean().item()\n",
        "        results[target_comp] = {\n",
        "            \"mse_all\": mse_all,\n",
        "            \"mse_by_pos\": mse_by_pos,\n",
        "            \"mse_context\": mse_context,\n",
        "            \"mse_target_cluster\": mse_target_cluster,\n",
        "        }\n",
        "        print(f\"Target component {target_comp}: MSE (all) = {mse_all:.6f}, MSE (context) = {mse_context:.6f}, MSE (target cluster) = {mse_target_cluster:.6f}\")\n",
        "\n",
        "    print(\"\\nDifference (target 1 − target 0):\")\n",
        "    print(f\"  MSE (all):           {results[1]['mse_all'] - results[0]['mse_all']:+.6f}\")\n",
        "    print(f\"  MSE (context):      {results[1]['mse_context'] - results[0]['mse_context']:+.6f}\")\n",
        "    print(f\"  MSE (target cluster): {results[1]['mse_target_cluster'] - results[0]['mse_target_cluster']:+.6f}\")\n",
        "\n",
        "    # Plot MSE by position for both\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
        "    ax.plot(results[0][\"mse_by_pos\"], \"o-\", label=\"Target component 0\", markersize=3)\n",
        "    ax.plot(results[1][\"mse_by_pos\"], \"s-\", label=\"Target component 1\", markersize=3)\n",
        "    ax.axvline(x=context_length - 0.5, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Context end\")\n",
        "    ax.set_xlabel(\"Position\")\n",
        "    ax.set_ylabel(\"MSE\")\n",
        "    ax.set_title(\"MSE by position: context [0,1] → target 0 vs target 1 (predict all)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipped (task is not group_mixture_linear).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8da65a7",
      "metadata": {},
      "source": [
        "# Probing with special inputs\n",
        "\n",
        "Feed the model **controlled inputs** to see what information the outputs reflect. All probes are **eval-only** (no training), and only for `group_mixture_linear`.\n",
        "\n",
        "- **Probe 1 – Same context & query x, different target component:** Use the same sequence of (x,y) in the context and the same query x at the end, but change *which component* is used for the target cluster (and thus the true y at the query). Compare the model’s prediction at the last position to the two possible true values. If the model has learned to use the target-cluster context to select the component, its prediction should track the correct component’s true y.\n",
        "- **Probe 2 – Same context & target component, vary query x:** Keep context and target component fixed; sweep the **query x** over several values (e.g. random or basis-aligned). For each query x, set the last position to that x and the true y to w^T x. Plot model prediction vs true y; if the model has learned the linear map, points should lie near the line y=x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1aca3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probing with special inputs (group_mixture_linear only; eval-only, no training changes)\n",
        "if conf.training.task != \"group_mixture_linear\":\n",
        "    print(\"Skipped (task is not group_mixture_linear).\")\n",
        "else:\n",
        "    # Sampler with predict-target-only for clean last-position readout\n",
        "    probe_task_kwargs = dict(task_kwargs)\n",
        "    probe_task_kwargs[\"predict_target_only\"] = True\n",
        "    probe_sampler = get_data_sampler(conf.training.data, n_dims=n_dims, **probe_task_kwargs)\n",
        "    probe_struct = probe_sampler.get_sequence_structure()\n",
        "    probe_inds = probe_struct[\"predict_inds\"]\n",
        "    n_pts_probe = probe_struct[\"total_length\"]\n",
        "    K_probe = probe_sampler.n_components\n",
        "    C_probe = probe_sampler.contexts_per_component\n",
        "    ctx_len = K_probe * C_probe\n",
        "    scale_probe = getattr(probe_sampler, \"scale\", 0.5)\n",
        "\n",
        "    # ----- Probe 1: Same xs, different target component (via different ys) -----\n",
        "    # One batch: fixed context [0,1], target=0\n",
        "    xs_one = probe_sampler.sample_xs(\n",
        "        n_points=n_pts_probe, b_size=batch_size,\n",
        "        fixed_cluster_assignments=[0, 1], fixed_target_component=0,\n",
        "    )\n",
        "    comps = probe_sampler.current_components\n",
        "    assign_0 = probe_sampler.component_assignments.clone()\n",
        "    assign_1 = assign_0.clone()\n",
        "    assign_1[:, ctx_len:] = 1\n",
        "    task_0 = task_sampler(components=comps, component_assignments=assign_0)\n",
        "    task_1 = task_sampler(components=comps, component_assignments=assign_1)\n",
        "    ys_0 = task_0.evaluate(xs_one)\n",
        "    ys_1 = task_1.evaluate(xs_one)\n",
        "    with torch.no_grad():\n",
        "        pred_0 = model(xs_one, ys_0, inds=probe_inds, sequence_structure=probe_struct)\n",
        "        pred_1 = model(xs_one, ys_1, inds=probe_inds, sequence_structure=probe_struct)\n",
        "    pred_0_last = pred_0[:, 0]\n",
        "    pred_1_last = pred_1[:, 0]\n",
        "    true_0_last = ys_0[:, -1]\n",
        "    true_1_last = ys_1[:, -1]\n",
        "\n",
        "    print(\"Probe 1: Same xs & query x, different target component (different ys in target cluster)\")\n",
        "    print(f\"  True y (target=0): mean={true_0_last.mean().item():.4f}, std={true_0_last.std().item():.4f}\")\n",
        "    print(f\"  True y (target=1): mean={true_1_last.mean().item():.4f}, std={true_1_last.std().item():.4f}\")\n",
        "    print(f\"  Pred when fed ys_0: mean={pred_0_last.mean().item():.4f}, std={pred_0_last.std().item():.4f}\")\n",
        "    print(f\"  Pred when fed ys_1: mean={pred_1_last.mean().item():.4f}, std={pred_1_last.std().item():.4f}\")\n",
        "    print(f\"  Correlation pred_0 vs true_0: {np.corrcoef(pred_0_last.cpu().numpy(), true_0_last.cpu().numpy())[0,1]:.4f}\")\n",
        "    print(f\"  Correlation pred_1 vs true_1: {np.corrcoef(pred_1_last.cpu().numpy(), true_1_last.cpu().numpy())[0,1]:.4f}\")\n",
        "\n",
        "    fig1, ax1 = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    ax1.scatter(true_0_last.cpu().numpy(), pred_0_last.cpu().numpy(), alpha=0.6, label=\"Fed ys_0 (target=0)\")\n",
        "    ax1.scatter(true_1_last.cpu().numpy(), pred_1_last.cpu().numpy(), alpha=0.6, label=\"Fed ys_1 (target=1)\")\n",
        "    mn = min(true_0_last.min().item(), true_1_last.min().item(), pred_0_last.min().item(), pred_1_last.min().item())\n",
        "    mx = max(true_0_last.max().item(), true_1_last.max().item(), pred_0_last.max().item(), pred_1_last.max().item())\n",
        "    ax1.plot([mn, mx], [mn, mx], \"k--\", alpha=0.5, label=\"y=x\")\n",
        "    ax1.set_xlabel(\"True y at query\"); ax1.set_ylabel(\"Model prediction at query\")\n",
        "    ax1.set_title(\"Probe 1: Prediction vs true y (same xs, different target component)\")\n",
        "    ax1.legend(); ax1.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "    # ----- Probe 2: Same context & target component, vary query x -----\n",
        "    n_queries = 20\n",
        "    # One fixed context, then expand to n_queries and only vary last-position x\n",
        "    xs_one_row = probe_sampler.sample_xs(\n",
        "        n_points=n_pts_probe, b_size=1,\n",
        "        fixed_cluster_assignments=[0, 1], fixed_target_component=0,\n",
        "    )\n",
        "    comps2 = probe_sampler.current_components.repeat(n_queries, 1, 1, 1)\n",
        "    assign2 = probe_sampler.component_assignments.repeat(n_queries, 1)\n",
        "    task2 = task_sampler(components=comps2, component_assignments=assign2)\n",
        "    xs_probe = xs_one_row.repeat(n_queries, 1, 1).clone()\n",
        "    query_xs = torch.randn(n_queries, n_dims)\n",
        "    xs_probe[:, -1, :] = query_xs\n",
        "    w_target = comps2[torch.arange(n_queries), assign2[:, -1], :, 0]\n",
        "    true_y_query = scale_probe * (query_xs * w_target).sum(dim=1)\n",
        "    ys_probe = task2.evaluate(xs_probe)\n",
        "    ys_probe[:, -1] = true_y_query.to(ys_probe.device)\n",
        "    with torch.no_grad():\n",
        "        pred_probe = model(xs_probe, ys_probe, inds=probe_inds, sequence_structure=probe_struct)[:, 0]\n",
        "    print(\"\\nProbe 2: Same context & target comp, vary query x\")\n",
        "    print(f\"  Correlation pred vs true y at query: {np.corrcoef(pred_probe.cpu().numpy(), true_y_query.numpy())[0,1]:.4f}\")\n",
        "\n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    ax2.scatter(true_y_query.numpy(), pred_probe.cpu().numpy(), alpha=0.8)\n",
        "    lims = [min(true_y_query.min().item(), pred_probe.min().item()), max(true_y_query.max().item(), pred_probe.max().item())]\n",
        "    ax2.plot(lims, lims, \"k--\", alpha=0.5, label=\"y=x\")\n",
        "    ax2.set_xlabel(\"True y = w^T x_query\"); ax2.set_ylabel(\"Model prediction\")\n",
        "    ax2.set_title(\"Probe 2: Vary query x (same context & target component)\")\n",
        "    ax2.legend(); ax2.grid(True, alpha=0.3); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4acd6a",
      "metadata": {},
      "source": [
        "# Test: Vary number of contexts per component (1 to 10) with zero-padding\n",
        "\n",
        "Model is trained on **10 contexts per component** (10-10-6 format). Here we feed it **i** contexts per component for i = 1, 2, ..., 10: we keep the same 10-10-6 sequence length, but for each context cluster we use only the first **i** positions as real (x,y) and **pad the rest with zeros** (x=0, y=0). The **target cluster (6 context + 1 query) is left unchanged** so the model always has full target-context signal. We measure MSE at the query position to see how performance depends on context size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa4e494",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vary contexts per component i in 1..10 with zero-padding; target cluster unchanged. Compare pred vs padded ys.\n",
        "if conf.training.task != \"group_mixture_linear\":\n",
        "    print(\"Skipped (task is not group_mixture_linear).\")\n",
        "else:\n",
        "    K = data_sampler.n_components\n",
        "    C_full = data_sampler.contexts_per_component  # 10\n",
        "    T_target = data_sampler.target_cluster_context_points  # 6\n",
        "    context_length = K * C_full  # 20\n",
        "    target_start = context_length\n",
        "    total_len = sequence_structure[\"total_length\"]  # 27\n",
        "\n",
        "    # One batch: full 10-10-6 data\n",
        "    xs_full = data_sampler.sample_xs(n_points=total_len, b_size=batch_size)\n",
        "    if hasattr(data_sampler, \"current_components\") and data_sampler.current_components is not None:\n",
        "        task_ctx = task_sampler(\n",
        "            components=data_sampler.current_components,\n",
        "            component_assignments=data_sampler.component_assignments,\n",
        "        )\n",
        "    else:\n",
        "        task_ctx = task_sampler()\n",
        "    ys_full = task_ctx.evaluate(xs_full)\n",
        "\n",
        "    # i = contexts per component (1..10); target cluster unchanged. Error = pred vs ys_pad (padded version).\n",
        "    mse_by_pos_per_i = []\n",
        "    for i in range(1, C_full + 1):\n",
        "        xs_pad = xs_full.clone()\n",
        "        ys_pad = ys_full.clone()\n",
        "        # Cluster 0: keep 0..i-1, zero i..9\n",
        "        xs_pad[:, i:C_full, :] = 0.0\n",
        "        ys_pad[:, i:C_full] = 0.0\n",
        "        # Cluster 1: keep 10..10+i-1, zero 10+i..19\n",
        "        xs_pad[:, C_full + i : context_length, :] = 0.0\n",
        "        ys_pad[:, C_full + i : context_length] = 0.0\n",
        "        # Target cluster (target_start..total_len-1) unchanged\n",
        "        with torch.no_grad():\n",
        "            pred = model(xs_pad, ys_pad)\n",
        "        sq_err = (pred - ys_pad.to(pred.device)) ** 2\n",
        "        mse_by_pos = sq_err.mean(dim=0).cpu().numpy()\n",
        "        mse_by_pos_per_i.append(mse_by_pos)\n",
        "        print(f\"  i={i}: MSE at query (vs padded) = {mse_by_pos[-1]:.6f}\")\n",
        "\n",
        "    positions = range(total_len)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "    for i in range(1, C_full + 1):\n",
        "        ax.plot(positions, mse_by_pos_per_i[i - 1], \"o-\", markersize=3, label=f\"i={i}\")\n",
        "    ax.axvline(x=context_length - 0.5, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Context end\")\n",
        "    ax.set_xlabel(\"Position\")\n",
        "    ax.set_ylabel(\"MSE (prediction error)\")\n",
        "    ax.set_title(\"Prediction error by position (i=1..10; compare pred vs padded ys)\")\n",
        "    ax.legend(loc=\"best\", ncol=2, fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c6b2a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = 3\n",
        "rows = (C_full + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(1, C_full + 1):\n",
        "    ax = axes[i-1]\n",
        "    # Plot ALL lines in light gray first (the \"ghost\" effect)\n",
        "    for other_mse in mse_by_pos_per_i:\n",
        "        ax.plot(positions, other_mse, color=\"gray\", alpha=0.1, lw=1)\n",
        "    \n",
        "    # Plot the specific line for this subplot in color\n",
        "    ax.plot(positions, mse_by_pos_per_i[i-1], \"o-\", markersize=3, color=\"blue\", label=f\"i={i}\")\n",
        "    ax.axvline(x=context_length - 0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
        "    ax.set_title(f\"Context i={i}\")\n",
        "\n",
        "# Clean up empty subplots\n",
        "for j in range(i, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be54bc7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/eddieqiao/Documents/mit/fall-2025/urop/in-context-learning/src/eval.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(state_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"DETAILED ERROR ANALYSIS BY POSITION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Compute squared errors\n",
        "if predict_inds is not None and len(predict_inds) > 0:\n",
        "    if len(predict_inds) == ys.shape[1]:\n",
        "        # All positions: pred and ys have same shape (B, T)\n",
        "        e = (pred - ys)**2  # (B, T)\n",
        "    else:\n",
        "        # Specific positions\n",
        "        e = (pred - ys[:, predict_inds])**2\n",
        "else:\n",
        "    e = (pred - ys)**2\n",
        "\n",
        "mse_by_pos = e.mean(dim=0)  # Average across batch\n",
        "print(f\"MSE by position: {mse_by_pos.tolist()}\")\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: MSE by position (line plot)\n",
        "if len(mse_by_pos.shape) == 0:\n",
        "    # Single value\n",
        "    axes[0, 0].bar([0], [mse_by_pos.item()])\n",
        "    axes[0, 0].set_xlabel(\"Position\")\n",
        "    axes[0, 0].set_ylabel(\"MSE\")\n",
        "    axes[0, 0].set_title(f'MSE at Position (Value: {mse_by_pos.item():.6f})')\n",
        "else:\n",
        "    positions = range(len(mse_by_pos))\n",
        "    axes[0, 0].plot(positions, mse_by_pos.cpu().numpy(), 'o-', linewidth=2, markersize=5)\n",
        "    axes[0, 0].set_xlabel(\"Position in Sequence\", fontsize=11)\n",
        "    axes[0, 0].set_ylabel(\"Mean Squared Error\", fontsize=11)\n",
        "    axes[0, 0].set_title(f'MSE by Position (Mean: {mse_by_pos.mean():.6f})', fontsize=12)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add boundary line for group_mixture_linear\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < len(positions):\n",
        "            axes[0, 0].axvline(x=context_length-0.5, color='r', linestyle='--', alpha=0.5)\n",
        "            axes[0, 0].text(context_length, mse_by_pos.max() * 0.9, 'Context/Target\\nboundary', \n",
        "                           ha='center', fontsize=9, color='r')\n",
        "\n",
        "# Plot 2: Error distribution histogram\n",
        "axes[0, 1].hist(loss.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].set_xlabel(\"Squared Error\", fontsize=11)\n",
        "axes[0, 1].set_ylabel(\"Frequency\", fontsize=11)\n",
        "axes[0, 1].set_title(f'Error Distribution (Mean: {loss.mean():.4f})', fontsize=12)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Heatmap of errors by example and position (if multiple positions)\n",
        "if len(e.shape) == 2 and e.shape[1] > 1:\n",
        "    # Show first 20 examples to avoid overcrowding\n",
        "    n_examples_show = min(20, e.shape[0])\n",
        "    im = axes[1, 0].imshow(e[:n_examples_show].cpu().numpy(), aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
        "    axes[1, 0].set_xlabel(\"Position in Sequence\", fontsize=11)\n",
        "    axes[1, 0].set_ylabel(\"Example Index\", fontsize=11)\n",
        "    axes[1, 0].set_title(f'Squared Error Heatmap (First {n_examples_show} Examples)', fontsize=12)\n",
        "    plt.colorbar(im, ax=axes[1, 0], label='Squared Error')\n",
        "    \n",
        "    # Add boundary line\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < e.shape[1]:\n",
        "            axes[1, 0].axvline(x=context_length-0.5, color='cyan', linestyle='--', linewidth=2, alpha=0.7)\n",
        "else:\n",
        "    axes[1, 0].axis('off')\n",
        "\n",
        "# Plot 4: Box plot of errors by position (if multiple positions)\n",
        "if len(mse_by_pos.shape) > 0 and len(mse_by_pos) > 1:\n",
        "    # Group positions for box plot (every 5 positions or so)\n",
        "    n_positions = len(mse_by_pos)\n",
        "    if n_positions > 20:\n",
        "        # Sample positions for readability\n",
        "        step = max(1, n_positions // 20)\n",
        "        sampled_positions = list(range(0, n_positions, step))\n",
        "        sampled_errors = [e[:, pos].cpu().numpy() for pos in sampled_positions]\n",
        "        axes[1, 1].boxplot(sampled_errors, labels=[f'P{p}' for p in sampled_positions])\n",
        "        axes[1, 1].set_xlabel(\"Position (sampled)\", fontsize=11)\n",
        "    else:\n",
        "        errors_by_pos = [e[:, pos].cpu().numpy() for pos in range(n_positions)]\n",
        "        axes[1, 1].boxplot(errors_by_pos, labels=[f'P{p}' for p in range(n_positions)])\n",
        "        axes[1, 1].set_xlabel(\"Position\", fontsize=11)\n",
        "    axes[1, 1].set_ylabel(\"Squared Error\", fontsize=11)\n",
        "    axes[1, 1].set_title('Error Distribution by Position', fontsize=12)\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOverall statistics:\")\n",
        "if len(mse_by_pos.shape) == 0:\n",
        "    print(f\"  MSE: {mse_by_pos.item():.6f}\")\n",
        "else:\n",
        "    print(f\"  Mean MSE across all positions: {mse_by_pos.mean():.6f}\")\n",
        "    print(f\"  Std MSE across positions: {mse_by_pos.std():.6f}\")\n",
        "    print(f\"  Min MSE (best position): {mse_by_pos.min():.6f} at position {mse_by_pos.argmin().item()}\")\n",
        "    print(f\"  Max MSE (worst position): {mse_by_pos.max():.6f} at position {mse_by_pos.argmax().item()}\")\n",
        "    \n",
        "    # Show performance in context vs target regions\n",
        "    if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "        K = data_sampler.n_components\n",
        "        C = data_sampler.contexts_per_component\n",
        "        context_length = K * C\n",
        "        if context_length < len(mse_by_pos):\n",
        "            context_mse = mse_by_pos[:context_length].mean()\n",
        "            target_mse = mse_by_pos[context_length:].mean()\n",
        "            print(f\"\\nPerformance by region:\")\n",
        "            print(f\"  Context clusters (0-{context_length-1}): Mean MSE = {context_mse:.6f}\")\n",
        "            print(f\"  Target cluster ({context_length}-{len(mse_by_pos)-1}): Mean MSE = {target_mse:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5fd9ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE VISUALIZATION: Performance at Every Index\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PERFORMANCE AT EVERY INDEX - DETAILED VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Compute errors for all positions\n",
        "if len(predict_inds) == ys.shape[1]:\n",
        "    # All positions predicted\n",
        "    e = (pred - ys)**2  # (B, T)\n",
        "    positions = list(range(ys.shape[1]))\n",
        "else:\n",
        "    # Specific positions\n",
        "    e = (pred - ys[:, predict_inds])**2\n",
        "    positions = predict_inds\n",
        "\n",
        "mse_by_pos = e.mean(dim=0).cpu().numpy()  # (T,)\n",
        "std_by_pos = e.std(dim=0).cpu().numpy()   # (T,)\n",
        "\n",
        "# Create comprehensive figure\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Plot 1: MSE by position with error bars (top left)\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "ax1.errorbar(positions, mse_by_pos, yerr=std_by_pos, \n",
        "            fmt='o-', linewidth=2, markersize=5, capsize=3, capthick=1.5)\n",
        "ax1.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax1.set_ylabel(\"Mean Squared Error\", fontsize=11)\n",
        "ax1.set_title('MSE by Position (with Std Dev)', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add context/target boundary if applicable\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax1.axvline(x=context_length-0.5, color='r', linestyle='--', linewidth=2, alpha=0.6)\n",
        "        ax1.text(context_length, mse_by_pos.max() * 0.95, 'Context/Target\\nBoundary', \n",
        "                ha='center', fontsize=9, color='r', fontweight='bold')\n",
        "\n",
        "# Plot 2: Log-scale MSE (top middle)\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "ax2.semilogy(positions, mse_by_pos, 'o-', linewidth=2, markersize=5)\n",
        "ax2.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax2.set_ylabel(\"MSE (log scale)\", fontsize=11)\n",
        "ax2.set_title('MSE by Position (Log Scale)', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, which='both')\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax2.axvline(x=context_length-0.5, color='r', linestyle='--', linewidth=2, alpha=0.6)\n",
        "\n",
        "# Plot 3: Relative error (MSE normalized by mean) (top right)\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "mean_mse = mse_by_pos.mean()\n",
        "relative_error = mse_by_pos / mean_mse\n",
        "ax3.plot(positions, relative_error, 'o-', linewidth=2, markersize=5, color='green')\n",
        "ax3.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Mean')\n",
        "ax3.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax3.set_ylabel(\"Relative MSE (vs Mean)\", fontsize=11)\n",
        "ax3.set_title('Relative Performance by Position', fontsize=12, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax3.axvline(x=context_length-0.5, color='r', linestyle='--', linewidth=2, alpha=0.6)\n",
        "\n",
        "# Plot 4: Error heatmap (bottom left, larger)\n",
        "ax4 = plt.subplot(2, 3, (4, 5))\n",
        "n_examples_show = min(30, e.shape[0])\n",
        "im = ax4.imshow(e[:n_examples_show].cpu().numpy(), aspect='auto', \n",
        "                cmap='YlOrRd', interpolation='nearest', vmin=0, vmax=e.max().item())\n",
        "ax4.set_xlabel(\"Position Index\", fontsize=11)\n",
        "ax4.set_ylabel(\"Example Index\", fontsize=11)\n",
        "ax4.set_title(f'Error Heatmap (First {n_examples_show} Examples)', fontsize=12, fontweight='bold')\n",
        "cbar = plt.colorbar(im, ax=ax4)\n",
        "cbar.set_label('Squared Error', fontsize=10)\n",
        "\n",
        "# Add boundary line\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        ax4.axvline(x=positions.index(context_length) if context_length in positions else context_length-0.5, \n",
        "                   color='cyan', linestyle='--', linewidth=2, alpha=0.8)\n",
        "\n",
        "# Plot 5: Box plot by position groups (bottom right)\n",
        "ax5 = plt.subplot(2, 3, 6)\n",
        "if len(positions) > 10:\n",
        "    # Group positions into bins\n",
        "    n_bins = min(10, len(positions))\n",
        "    bin_size = len(positions) // n_bins\n",
        "    position_groups = []\n",
        "    group_labels = []\n",
        "    for i in range(n_bins):\n",
        "        start_idx = i * bin_size\n",
        "        end_idx = (i + 1) * bin_size if i < n_bins - 1 else len(positions)\n",
        "        group_positions = positions[start_idx:end_idx]\n",
        "        group_errors = [e[:, pos].cpu().numpy() for pos in group_positions]\n",
        "        position_groups.append(group_errors)\n",
        "        group_labels.append(f'{start_idx}-{end_idx-1}')\n",
        "    \n",
        "    # Flatten for box plot\n",
        "    box_data = []\n",
        "    box_labels = []\n",
        "    for group_errors, label in zip(position_groups, group_labels):\n",
        "        for pos_errors in group_errors:\n",
        "            box_data.append(pos_errors)\n",
        "            box_labels.append(label)\n",
        "    \n",
        "    bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "else:\n",
        "    # Show all positions\n",
        "    box_data = [e[:, pos].cpu().numpy() for pos in positions]\n",
        "    box_labels = [f'P{p}' for p in positions]\n",
        "    bp = ax5.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "ax5.set_xlabel(\"Position Range\", fontsize=11)\n",
        "ax5.set_ylabel(\"Squared Error\", fontsize=11)\n",
        "ax5.set_title('Error Distribution by Position Group', fontsize=12, fontweight='bold')\n",
        "ax5.tick_params(axis='x', rotation=45)\n",
        "ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total positions evaluated: {len(positions)}\")\n",
        "print(f\"Mean MSE across all positions: {mse_by_pos.mean():.6f}\")\n",
        "print(f\"Std MSE across positions: {mse_by_pos.std():.6f}\")\n",
        "print(f\"Min MSE: {mse_by_pos.min():.6f} at position {positions[np.argmin(mse_by_pos)]}\")\n",
        "print(f\"Max MSE: {mse_by_pos.max():.6f} at position {positions[np.argmax(mse_by_pos)]}\")\n",
        "print(f\"Median MSE: {np.median(mse_by_pos):.6f}\")\n",
        "\n",
        "# Performance by region (if applicable)\n",
        "if conf.training.task == \"group_mixture_linear\" and hasattr(data_sampler, 'contexts_per_component'):\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    context_length = K * C\n",
        "    if context_length < len(positions):\n",
        "        context_positions = [p for p in positions if p < context_length]\n",
        "        target_positions = [p for p in positions if p >= context_length]\n",
        "        \n",
        "        if context_positions:\n",
        "            context_indices = [positions.index(p) for p in context_positions]\n",
        "            context_mse = mse_by_pos[context_indices].mean()\n",
        "            print(f\"\\nContext Clusters (positions 0-{context_length-1}):\")\n",
        "            print(f\"  Mean MSE: {context_mse:.6f}\")\n",
        "            print(f\"  Positions: {len(context_positions)}\")\n",
        "        \n",
        "        if target_positions:\n",
        "            target_indices = [positions.index(p) for p in target_positions]\n",
        "            target_mse = mse_by_pos[target_indices].mean()\n",
        "            print(f\"\\nTarget Cluster (positions {context_length}-{len(positions)-1}):\")\n",
        "            print(f\"  Mean MSE: {target_mse:.6f}\")\n",
        "            print(f\"  Positions: {len(target_positions)}\")\n",
        "            print(f\"  Ratio (target/context): {target_mse/context_mse:.3f}\" if context_positions else \"\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea393db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs stats: mean=-0.0017, std=0.5688, min=-3.4024, max=3.4349\n",
            "ys stats: mean=-0.0052, std=0.1817, min=-0.8882, max=0.7165\n",
            "coefficients[0]: tensor([ 0.0216,  0.0845, -0.0595,  0.0055, -0.0728,  0.1485,  0.0060, -0.0074,\n",
            "        -0.0203,  0.0035])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.7556, -0.0793,  0.7708, -0.8922, -0.4725,  0.5542, -1.1830,  0.5065,\n",
              "           0.2305, -0.2648],\n",
              "         [-0.2118, -0.7556, -0.0793,  0.7708, -0.8922, -0.4725,  0.5542, -1.1830,\n",
              "           0.5065,  0.2305],\n",
              "         [ 0.1393, -0.2118, -0.7556, -0.0793,  0.7708, -0.8922, -0.4725,  0.5542,\n",
              "          -1.1830,  0.5065],\n",
              "         [-0.3126,  0.1393, -0.2118, -0.7556, -0.0793,  0.7708, -0.8922, -0.4725,\n",
              "           0.5542, -1.1830],\n",
              "         [ 0.1181, -0.3126,  0.1393, -0.2118, -0.7556, -0.0793,  0.7708, -0.8922,\n",
              "          -0.4725,  0.5542],\n",
              "         [ 0.3356,  0.1181, -0.3126,  0.1393, -0.2118, -0.7556, -0.0793,  0.7708,\n",
              "          -0.8922, -0.4725],\n",
              "         [-0.2420,  0.3356,  0.1181, -0.3126,  0.1393, -0.2118, -0.7556, -0.0793,\n",
              "           0.7708, -0.8922],\n",
              "         [-0.1926, -0.2420,  0.3356,  0.1181, -0.3126,  0.1393, -0.2118, -0.7556,\n",
              "          -0.0793,  0.7708],\n",
              "         [-0.0583, -0.1926, -0.2420,  0.3356,  0.1181, -0.3126,  0.1393, -0.2118,\n",
              "          -0.7556, -0.0793],\n",
              "         [-0.0018, -0.0583, -0.1926, -0.2420,  0.3356,  0.1181, -0.3126,  0.1393,\n",
              "          -0.2118, -0.7556],\n",
              "         [ 0.0166, -0.0018, -0.0583, -0.1926, -0.2420,  0.3356,  0.1181, -0.3126,\n",
              "           0.1393, -0.2118],\n",
              "         [-0.0088,  0.0166, -0.0018, -0.0583, -0.1926, -0.2420,  0.3356,  0.1181,\n",
              "          -0.3126,  0.1393],\n",
              "         [ 0.2592, -0.0088,  0.0166, -0.0018, -0.0583, -0.1926, -0.2420,  0.3356,\n",
              "           0.1181, -0.3126],\n",
              "         [ 0.2235,  0.2592, -0.0088,  0.0166, -0.0018, -0.0583, -0.1926, -0.2420,\n",
              "           0.3356,  0.1181],\n",
              "         [-0.0321,  0.2235,  0.2592, -0.0088,  0.0166, -0.0018, -0.0583, -0.1926,\n",
              "          -0.2420,  0.3356],\n",
              "         [ 0.1221, -0.0321,  0.2235,  0.2592, -0.0088,  0.0166, -0.0018, -0.0583,\n",
              "          -0.1926, -0.2420],\n",
              "         [-0.2547,  0.1221, -0.0321,  0.2235,  0.2592, -0.0088,  0.0166, -0.0018,\n",
              "          -0.0583, -0.1926],\n",
              "         [-0.0416, -0.2547,  0.1221, -0.0321,  0.2235,  0.2592, -0.0088,  0.0166,\n",
              "          -0.0018, -0.0583],\n",
              "         [-0.0500, -0.0416, -0.2547,  0.1221, -0.0321,  0.2235,  0.2592, -0.0088,\n",
              "           0.0166, -0.0018],\n",
              "         [ 0.1471, -0.0500, -0.0416, -0.2547,  0.1221, -0.0321,  0.2235,  0.2592,\n",
              "          -0.0088,  0.0166]]),\n",
              " tensor([ 0.0264, -0.0621, -0.1401,  0.1163,  0.0327, -0.0498, -0.0500,  0.0081,\n",
              "         -0.0388, -0.0030,  0.0695, -0.0129, -0.0279,  0.0130,  0.0084, -0.0053,\n",
              "         -0.0116, -0.0080,  0.0480, -0.0140]))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"MODEL CAPABILITY ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if conf.training.task == \"group_mixture_linear\":\n",
        "    print(\"On-the-fly Mixture Linear Regression Analysis:\")\n",
        "    K = data_sampler.n_components\n",
        "    C = data_sampler.contexts_per_component\n",
        "    T_target = data_sampler.target_cluster_context_points\n",
        "    context_length = K * C\n",
        "    total_length = sequence_structure['total_length'] if sequence_structure else n_points\n",
        "    \n",
        "    print(f\"  - Task structure:\")\n",
        "    print(f\"    * {K} mixture components\")\n",
        "    print(f\"    * {K} context clusters, each with {C} points\")\n",
        "    print(f\"    * 1 target cluster with {T_target} context points + 1 prediction point\")\n",
        "    print(f\"    * Total sequence length: {total_length}\")\n",
        "    print(f\"  - Model must:\")\n",
        "    print(f\"    1. Learn {K} weight vectors from context clusters (positions 0-{context_length-1})\")\n",
        "    print(f\"    2. Infer which component is used in target cluster from first {T_target} target points\")\n",
        "    print(f\"    3. Predict ALL {total_length} positions using the learned components\")\n",
        "    print(f\"  - Clusters are in fixed order (no randomization)\")\n",
        "    print(f\"  - All components guaranteed to appear in context clusters\")\n",
        "    \n",
        "    # Analyze component usage\n",
        "    if hasattr(data_sampler, 'component_assignments'):\n",
        "        print(f\"\\n  - Component usage in this batch:\")\n",
        "        for comp_id in range(K):\n",
        "            comp_count = (data_sampler.component_assignments == comp_id).sum().item()\n",
        "            print(f\"    Component {comp_id}: {comp_count} positions\")\n",
        "        \n",
        "        # Check if target components appeared in context\n",
        "        if hasattr(data_sampler, 'target_components'):\n",
        "            target_in_context = 0\n",
        "            for b in range(batch_size):\n",
        "                target_comp = data_sampler.target_components[b].item()\n",
        "                context_comps = set(data_sampler.component_assignments[b, :context_length].cpu().tolist())\n",
        "                if target_comp in context_comps:\n",
        "                    target_in_context += 1\n",
        "            print(f\"  - Target component appeared in context for {target_in_context}/{batch_size} examples\")\n",
        "\n",
        "elif predict_inds is not None and sequence_structure is not None:\n",
        "    print(\"Multi-context learning analysis:\")\n",
        "    if 'n_contexts' in sequence_structure:\n",
        "        print(f\"  - Model sees {sequence_structure['n_contexts']} context series\")\n",
        "        print(f\"  - Each context has {sequence_structure['context_length']} points\") \n",
        "        print(f\"  - Predicts {sequence_structure['predict_length']} future points\")\n",
        "    else:\n",
        "        print(f\"  - Sequence structure: {sequence_structure}\")\n",
        "        print(f\"  - Predicting {len(predict_inds)} positions\")\n",
        "    \n",
        "    if hasattr(task, 'get_mixture_info'):\n",
        "        mix_info = task.get_mixture_info()\n",
        "        print(f\"  - Must identify correct component from {mix_info['n_components']} possibilities\")\n",
        "        \n",
        "        correct_identification = 0\n",
        "        total = 0\n",
        "        \n",
        "        for b in range(batch_size):\n",
        "            target_comp = mix_info['target_assignment'][b]\n",
        "            context_comps = mix_info['context_assignments'][b]\n",
        "            \n",
        "            if target_comp in context_comps:\n",
        "                total += 1\n",
        "                \n",
        "        if total > 0:\n",
        "            print(f\"  - Target component appeared in context for {total}/{batch_size} examples\")\n",
        "\n",
        "else:\n",
        "    print(\"Standard in-context learning analysis:\")\n",
        "    print(f\"  - Model learns from increasing context (up to {n_points} points)\")\n",
        "    print(f\"  - Task: {conf.training.task}\")\n",
        "    print(f\"  - Data: {conf.training.data}\")\n",
        "    if predict_inds is not None:\n",
        "        print(f\"  - Predicting {len(predict_inds)} positions\")\n",
        "    else:\n",
        "        print(f\"  - Predicting all positions (autoregressive)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394cc45d",
      "metadata": {},
      "source": [
        "# ✅ FIX APPLIED\n",
        "\n",
        "## The Bug and the Fix\n",
        "\n",
        "**The Bug:** The targets (ys) were **noiseless predictions** instead of actual noisy values.\n",
        "- Before: `ys = task.evaluate(xs)` computed ys as dot products (no noise!)\n",
        "- This allowed MSE < noise_variance (0.04) because targets had no noise\n",
        "\n",
        "**The Fix:**\n",
        "1. **Modified `ARWarmupSampler.sample_xs()`** to store actual noisy next values in `self.current_ys`\n",
        "2. **Modified training loop** to use `data_sampler.current_ys` instead of `task.evaluate(xs)` for AR tasks\n",
        "3. **Need to update eval cell 6** - replace `ys = task.evaluate(xs)` with:\n",
        "   ```python\n",
        "   if hasattr(data_sampler, 'current_ys'):\n",
        "       ys = data_sampler.current_ys\n",
        "   else:\n",
        "       ys = task.evaluate(xs)\n",
        "   ```\n",
        "\n",
        "**After the fix:**\n",
        "- ys will contain the actual noisy values from the AR sequence\n",
        "- Theoretical minimum MSE will correctly be 0.04 (noise variance)\n",
        "- The model will be learning to predict noisy targets, which is the correct formulation\n",
        "\n",
        "**To apply:** \n",
        "1. Restart the kernel\n",
        "2. Re-run cells from the beginning\n",
        "3. You should now see MSE values that make sense relative to the noise floor!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae10a8c2",
      "metadata": {},
      "source": [
        "# Manual Update Required for Cell 6\n",
        "\n",
        "**Before running the evaluation, update the cell where `ys = task.evaluate(xs)` appears:**\n",
        "\n",
        "### Find this code (around cell 5 or 6):\n",
        "```python\n",
        "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
        "\n",
        "task_sampler_args = {\"coefficients\": data_sampler.current_coefficients}\n",
        "task = task_sampler(**task_sampler_args)\n",
        "ys = task.evaluate(xs)  # ← OLD (noiseless)\n",
        "```\n",
        "\n",
        "### Replace with:\n",
        "```python\n",
        "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
        "\n",
        "task_sampler_args = {\"coefficients\": data_sampler.current_coefficients}\n",
        "task = task_sampler(**task_sampler_args)\n",
        "\n",
        "# Use actual noisy targets (includes noise from AR generation)\n",
        "if hasattr(data_sampler, 'current_ys'):\n",
        "    ys = data_sampler.current_ys\n",
        "    print(\"✓ Using actual noisy targets\")\n",
        "else:\n",
        "    ys = task.evaluate(xs)\n",
        "    print(\"⚠️ Falling back to noiseless predictions\")\n",
        "```\n",
        "\n",
        "After this change, **restart the kernel and re-run from the beginning!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "in-context-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
